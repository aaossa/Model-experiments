{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets.utils import PreprocessingDataset\n",
    "from models.utils import get_model_by_name\n",
    "from utils.environment import modified_environ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE, NUM_WORKERS = 8, 4\n",
    "IMAGES_EXT = [\"*.gif\", \"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]\n",
    "USE_GPU = True\n",
    "\n",
    "# Dataset\n",
    "DATASET = \"UGallery\"\n",
    "assert DATASET in [\"UGallery\", \"Wikimedia\", \"Pinterest\"]\n",
    "\n",
    "# Model\n",
    "MODEL = \"resnet50\"\n",
    "assert MODEL in [\"resnet50\"]\n",
    "MODEL_VERSION = \"imagenet\"  # \"imagenet\" or \"places365\"\n",
    "assert MODEL_VERSION in [\"imagenet\", \"places365\"]\n",
    "\n",
    "# Images path\n",
    "IMAGES_DIR = None\n",
    "if DATASET == \"UGallery\":\n",
    "    # ~35s, v2 27.6s\n",
    "    # IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"workspace\", \"Ugallery\", \"images\")\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"workspace\", \"Ugallery\", \"mini-images-224-224-v2\")\n",
    "elif DATASET == \"Wikimedia\":\n",
    "    # ~1h 10m, v2 1m 36s\n",
    "    # IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"wikimedia\", \"images\", \"img\")\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"wikimedia\", \"mini-images-224-224-v2\")\n",
    "elif DATASET == \"Pinterest\":\n",
    "    # ~2h, v2 1h 7m\n",
    "    # IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"pinterest_iccv\", \"images\")\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"pinterest_iccv\", \"mini-images_filtered-224-224-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (output)\n",
    "OUTPUT_EMBEDDING_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}_embedding-{MODEL}_{MODEL_VERSION}.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import ImageFile\n",
    "\n",
    "\n",
    "# Needed for some images in Pinterest and Wikimedia dataset\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 3_000_000_000\n",
    "# Some images are \"broken\" in Wikimedia dataset\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Setting up torch device (useful if GPU available)\n",
    "print(\"\\nCreating device...\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "if torch.cuda.is_available() != USE_GPU:\n",
    "    print((f\"\\nNotice: Not using GPU - \"\n",
    "           f\"Cuda available ({torch.cuda.is_available()}) \"\n",
    "           f\"does not match USE_GPU ({USE_GPU})\"\n",
    "    ))\n",
    "\n",
    "# Downloading models for feature extraction\n",
    "print(\"\\nDownloading model...\")\n",
    "with modified_environ(TORCH_HOME=\".\"):\n",
    "    print(f\"Model: {MODEL} (pretrained on {MODEL_VERSION})\")\n",
    "    if MODEL_VERSION == \"imagenet\":\n",
    "        model = get_model_by_name(MODEL).eval().to(device)\n",
    "    else:\n",
    "        # Places365\n",
    "        model = torchvision.models.__dict__[\"resnet50\"](num_classes=365)\n",
    "        checkpoint = torch.load(os.path.join(\"/\", \"mnt\", \"data2\", \"netdissect2\", \"lite\", \"zoo\", \"resnet50_places365.pth.tar\"))\n",
    "        model.load_state_dict({\n",
    "            str.replace(k, 'module.', ''): v for k, v in checkpoint['state_dict'].items()\n",
    "        })\n",
    "        del checkpoint\n",
    "        # Drop last layer (need embedding, not classification)\n",
    "        model = torch.nn.Sequential(*list(model.children()))[:-1]\n",
    "        for param in model.parameters():\n",
    "            model.requires_grad = False\n",
    "        model = model.eval().to(device)\n",
    "\n",
    "# Setting up transforms and dataset\n",
    "print(\"\\nSetting up transforms and dataset...\")\n",
    "images_transforms = transforms.Compose([\n",
    "    # transforms.Resize(256),  # Already done in mini v2\n",
    "    # transforms.CenterCrop(224),  # Already done in mini v2\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image_dataset = PreprocessingDataset(\n",
    "    IMAGES_DIR,\n",
    "    extensions=IMAGES_EXT,\n",
    "    transform=images_transforms,\n",
    ")\n",
    "image_dataloader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "print(f\">> Images dataset: {len(image_dataset)}\")\n",
    "\n",
    "# Calculate embedding dimension size\n",
    "dummy_input = torch.ones(1, *image_dataset[0][\"image\"].size()).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "emb_dim = dummy_output.size(1)\n",
    "print(f\">> Embedding dimension size: {emb_dim}\")\n",
    "\n",
    "# Feature extraction phase\n",
    "print(f\"\\nFeature extraction...\")\n",
    "output_ids = np.empty(len(image_dataset), dtype=object)\n",
    "output_embedding = torch.zeros((len(image_dataset), emb_dim), dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for batch_i, sample in enumerate(tqdm(image_dataloader, desc=\"Feature extraction\")):\n",
    "        item_image = sample[\"image\"].to(device)\n",
    "        item_idx = sample[\"idx\"]\n",
    "        output_ids[[*item_idx]] = sample[\"id\"]\n",
    "        output_embedding[item_idx] = model(item_image).squeeze(-1).squeeze(-1)\n",
    "output_embedding = output_embedding.cpu().numpy()\n",
    "\n",
    "# Fill output embedding\n",
    "embedding = np.ndarray(\n",
    "    shape=(len(image_dataset), 2),\n",
    "    dtype=object,\n",
    ")\n",
    "for i in range(len(image_dataset)):\n",
    "    embedding[i] = np.asarray([output_ids[i], output_embedding[i]])\n",
    "print(f\">> Embedding shape: {embedding.shape}\")\n",
    "\n",
    "# Save embedding to file\n",
    "print(f\"\\nSaving embedding to file... ({OUTPUT_EMBEDDING_PATH})\")\n",
    "np.save(OUTPUT_EMBEDDING_PATH, embedding, allow_pickle=True)\n",
    "\n",
    "# Free some memory\n",
    "if USE_GPU:\n",
    "    print(f\"\\nCleaning GPU cache...\")\n",
    "    model = model.to(torch.device(\"cpu\"))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Finished\n",
    "print(\"\\nDone\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.5",
   "language": "python",
   "name": "3.8.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
