{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-da2aa7eb68ef>:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from utils.ugallery.data import (\n",
    "    get_transactions_dataframes, add_aggregation_columns,\n",
    "    mark_evaluation_rows, get_holdout, map_ids_to_indexes,\n",
    ")\n",
    "from utils.ugallery.hashing import pre_hash, HashesContainer\n",
    "from utils.ugallery.sampling import StrategyHandler\n",
    "from utils.ugallery.similarity import HybridScorer, VisualSimilarityHandler\n",
    "\n",
    "\n",
    "# Parameters\n",
    "RNG_SEED = 0\n",
    "EMBEDDING_FN = os.path.join(\"data\", \"UGallery\", \"ugallery_embedding.npy\")\n",
    "PCA_COMPONENTS = 200\n",
    "CLUSTERING_RNG = None\n",
    "CLUSTERING_N_CLUSTERS = 100\n",
    "CLUSTERING_N_TIMES = 5  # 20\n",
    "CLUSTERING_N_INIT = 1  # 8\n",
    "CLUSTERING_N_JOBS = os.cpu_count()  # 4 or 8\n",
    "INVENTORY_PATH = os.path.join(\"data\", \"UGallery\", \"valid_artworks.csv\")\n",
    "PURCHASES_PATH = os.path.join(\"data\", \"UGallery\", \"valid_sales.csv\")\n",
    "OUTPUT_TRAIN_PATH = os.path.join(\"data\", \"UGallery\", \"train.csv\")\n",
    "OUTPUT_VALID_PATH = os.path.join(\"data\", \"UGallery\", \"validation.csv\")\n",
    "OUTPUT_EVAL_PATH = os.path.join(\"data\", \"UGallery\", \"evaluation.csv\")\n",
    "\n",
    "# Parameters (sampling)\n",
    "ARTIST_BOOST = 0.2\n",
    "CONFIDENCE_MARGIN = 0.18\n",
    "FINE_GRAINED_THRESHOLD = 0.7\n",
    "FAKE_COEF = 0.\n",
    "assert all(\n",
    "    0. <= var <= 1.\n",
    "    for var in [ARTIST_BOOST, CONFIDENCE_MARGIN, FINE_GRAINED_THRESHOLD, FAKE_COEF]\n",
    ")\n",
    "TOTAL_SAMPLES_TRAIN = 10_000_000\n",
    "TOTAL_SAMPLES_VALID = 500_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using random seed...\n",
      "\n",
      "Loading embedding from file... (data/UGallery/ugallery_embedding.npy)\n",
      "\n",
      "Extracting data into variables...\n",
      ">> Features shape: (13297, 4096)\n",
      "\n",
      "Creating visual clusters: 0. z-score normalization of embedding\n",
      ">> Features shape: (13297, 4096)\n",
      "\n",
      "Creating visual clusters: 1. Conduct PCA to reduce dimension\n",
      ">> Features shape: (13297, 200)\n",
      "\n",
      "Creating visual clusters: 2. Perform k-means clustering\n",
      ">> Best Silhouette score: 0.01127261796461595113)\n",
      "\n",
      "Loading transactions from files...\n",
      ">> Inventory: (7742, 3) | Purchases: (4897, 3)\n",
      ">> Mapping applied\n",
      ">> Purchases: (4897, 5)\n",
      ">> Purchases: (4897, 6)\n",
      ">> Evaluation: (728, 4) | Purchases: (4169, 6)\n",
      ">> Purchases: (4169, 6)\n",
      ">> Inventory: (7742, 4)\n",
      "\n",
      "Creating mappings from index to data\n",
      "\n",
      "Creating mappings from data to index\n",
      "\n",
      "Creating helpers instances...\n",
      "\n",
      "Calculating important values...\n",
      ">> There are 2 real strategies and 2 fake strategies\n",
      ">> N_USERS = 2919 | N_ITEMS = 13297\n",
      "\n",
      "Creating samples using custom strategies\n",
      ">> Strategy #1: Given real profile, recommend profile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8394f158c1421bb330370cfab2e5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 1', max=2919.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61456ca37e5040368c1c8a5a9484cefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 1', max=2919.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Strategy #1 Training samples (5000247) and validation samples (251034)\n",
      ">> Strategy #2: Given fake profile, recommend profile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e1a48c9e7542b2bdf3b678d7af1fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 2', max=13297.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39ddf375973445e806d3d9045a295f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 2', max=13297.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Strategy #2: Training samples (0) and validation samples (0)\n",
      ">> Strategy #3: Given real profile, recommend items according to hybrid scorer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770e6298a4d48af924857d524b2a181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 3', max=2919.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d398537bd364804bde1bdf4dec5c4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 3', max=2919.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Strategy #3: Training samples (5000247) and validation samples (251034)\n",
      ">> Strategy #4: Given fake profile, recommend items according to hybrid scorer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a359e2ce5e954774b1381135abccfc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 4', max=13297.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc573fc19b047c1ae36850717e8f107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Strategy 4', max=13297.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Strategy #4: Training samples (0) and validation samples (0)\n",
      ">> Total hash collisions: 11542792\n",
      ">> Total visual collisions: 1134\n",
      "\n",
      "Merging strategies samples into a single list\n",
      ">> Strategy 1: Size: 5000247 | Sample: (array([  409, 13294]), 13294, 1610, 269)\n",
      ">> Strategy 2: Size: 0000000 | Sample: None\n",
      ">> Strategy 3: Size: 5000247 | Sample: (array([  409, 13294]), 9969, 5248, 269)\n",
      ">> Strategy 4: Size: 0000000 | Sample: None\n",
      ">> Training samples: 10000494\n",
      ">> Strategy 1: Size: 0251034 | Sample: (array([  409, 13294]), 13294, 11918, 269)\n",
      ">> Strategy 2: Size: 0000000 | Sample: None\n",
      ">> Strategy 3: Size: 0251034 | Sample: (array([  409, 13294]), 12811, 12471, 269)\n",
      ">> Strategy 4: Size: 0000000 | Sample: None\n",
      ">> Validation samples: 502068\n",
      "\n",
      "Naive triples validation and looking for duplicates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b20100407e444aa87fd951049a30232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Naive validation', max=10502562.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> No duped hashes found\n",
      "\n",
      "Creating output files (train and valid)...\n",
      ">> Saving training samples (data/UGallery/train.csv)\n",
      ">> Saving validation samples in (data/UGallery/validation.csv)\n",
      "\n",
      "Creating output files (evaluation)...\n",
      ">> Saving evaluation data in (data/UGallery/validation.csv)\n",
      "\n",
      "Done\n",
      "CPU times: user 11min 38s, sys: 1min 40s, total: 13min 18s\n",
      "Wall time: 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed...\")\n",
    "    random.seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n",
    "\n",
    "\n",
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_FN})\")\n",
    "embedding = np.load(EMBEDDING_FN, allow_pickle=True)\n",
    "\n",
    "\n",
    "# Extract features and id2index\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features = np.zeros(shape=(embedding.shape[0], embedding[0, 1].shape[0]))\n",
    "id2index = dict()\n",
    "for i, (_id, vector_embedding) in enumerate(embedding):\n",
    "    features[i] = vector_embedding\n",
    "    id2index[str(_id)] = i\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "\n",
    "# Creating visual clusters\n",
    "print(\"\\nCreating visual clusters: 0. z-score normalization of embedding\")\n",
    "features = StandardScaler().fit_transform(features)\n",
    "print(f\">> Features shape: {features.shape}\") ###\n",
    "\n",
    "print(\"\\nCreating visual clusters: 1. Conduct PCA to reduce dimension\")\n",
    "features = PCA(n_components=PCA_COMPONENTS).fit_transform(features)\n",
    "print(f\">> Features shape: {features.shape}\") ###\n",
    "\n",
    "\n",
    "print(\"\\nCreating visual clusters: 2. Perform k-means clustering\")\n",
    "best_score = float(\"-inf\")\n",
    "best_clusterer = None\n",
    "for i in range(CLUSTERING_N_TIMES):\n",
    "    clusterer = KMeans(\n",
    "        n_clusters=CLUSTERING_N_CLUSTERS,\n",
    "        max_iter=2000,\n",
    "        n_init=CLUSTERING_N_INIT,\n",
    "        n_jobs=CLUSTERING_N_JOBS,\n",
    "        random_state=CLUSTERING_RNG,\n",
    "    ).fit(features)\n",
    "    score = silhouette_score(features, clusterer.labels_)\n",
    "    if score > best_score:\n",
    "        best_clusterer = clusterer\n",
    "        best_score = score\n",
    "    if CLUSTERING_RNG is not None:\n",
    "        break\n",
    "    print((f\">> Silhouette score ({i + 1:02}/{CLUSTERING_N_TIMES}): \"\n",
    "           f\"{score:.4f} (Best: {best_score:.4f})\"), flush=True, end=\"\\r\")\n",
    "print(f\">> Best Silhouette score: {best_score}\")\n",
    "\n",
    "\n",
    "# Load transactions CSVs\n",
    "print(f\"\\nLoading transactions from files...\")\n",
    "inventory_df, purchases_df = get_transactions_dataframes(\n",
    "    INVENTORY_PATH, PURCHASES_PATH,\n",
    "    display_stats=False,\n",
    ")\n",
    "# Check if every purchased artwork is present in inventory\n",
    "for artwork_id in purchases_df[\"artwork_id\"].sum():\n",
    "    assert artwork_id in inventory_df[\"artwork_id\"].values\n",
    "print(f\">> Inventory: {inventory_df.shape} | Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Apply id2index, to work with indexes only\n",
    "inventory_df = map_ids_to_indexes(inventory_df, id2index)\n",
    "purchases_df = map_ids_to_indexes(purchases_df, id2index)\n",
    "print(\">> Mapping applied\")\n",
    "\n",
    "# Calculating number of baskets and size of each basket for purchases\n",
    "purchases_df = add_aggregation_columns(purchases_df)\n",
    "# Check if new values are reasonable\n",
    "assert all(purchases_df[\"n_baskets\"] > 0)\n",
    "assert all(purchases_df[\"n_items\"] > 0)\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Mark purchases used for evaluation procedure\n",
    "purchases_df = mark_evaluation_rows(purchases_df)\n",
    "# Check if new column exists and has boolean dtype\n",
    "assert purchases_df[\"evaluation\"].dtype.name == \"bool\"\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Split purchases data according to evaluation column\n",
    "evaluation_df, purchases_df = get_holdout(purchases_df)\n",
    "assert not purchases_df.empty\n",
    "assert not evaluation_df.empty\n",
    "print(f\">> Evaluation: {evaluation_df.shape} | Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Recalculate number of baskets and size of each basket for purchases\n",
    "purchases_df = add_aggregation_columns(purchases_df)\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Add cluster id information\n",
    "inventory_df[\"cluster_id\"] = inventory_df[\"artwork_id\"].apply(\n",
    "    lambda idx: best_clusterer.labels_[idx],\n",
    ")\n",
    "print(f\">> Inventory: {inventory_df.shape}\")\n",
    "\n",
    "\n",
    "# Create helper mapping from idx to data\n",
    "print(\"\\nCreating mappings from index to data\")\n",
    "artist_by_idx = np.full((features.shape[0],), -1)\n",
    "for artwork_id, artist_id in inventory_df.set_index(\"artwork_id\").to_dict()[\"artist_id\"].items():\n",
    "    artist_by_idx[artwork_id] = artist_id\n",
    "cluster_by_idx = best_clusterer.labels_\n",
    "\n",
    "\n",
    "# Create helper mapping from data to idxs\n",
    "print(\"\\nCreating mappings from data to index\")\n",
    "artistId2artworkIndexes = inventory_df.groupby(\"artist_id\")[\"artwork_id\"].apply(list).to_dict()\n",
    "clustId2artIndexes = dict()\n",
    "for i, cluster in enumerate(cluster_by_idx):\n",
    "    if cluster not in clustId2artIndexes:\n",
    "        clustId2artIndexes[cluster] = list()\n",
    "    clustId2artIndexes[cluster].append(i)\n",
    "\n",
    "\n",
    "print(\"\\nCreating helpers instances...\")\n",
    "# Creating hashes container for duplicates detection\n",
    "hashes_container = HashesContainer()\n",
    "# Creating custom score helpers\n",
    "vissimhandler = VisualSimilarityHandler(best_clusterer.labels_, features)\n",
    "hybrid_scorer = HybridScorer(vissimhandler, artist_by_idx, artist_boost=ARTIST_BOOST)\n",
    "\n",
    "\n",
    "# Sampling constants\n",
    "print(\"\\nCalculating important values...\")\n",
    "N_REAL_STRATEGIES = 2\n",
    "N_FAKE_STRATEGIES = 2\n",
    "print(f\">> There are {N_REAL_STRATEGIES} real strategies and {N_FAKE_STRATEGIES} fake strategies\")\n",
    "N_SAMPLES_PER_REAL_STRAT_TRAIN = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_TRAIN / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_REAL_STRAT_VALID = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_VALID / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_TRAIN = ceil(FAKE_COEF * TOTAL_SAMPLES_TRAIN / N_FAKE_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_VALID = ceil(FAKE_COEF * TOTAL_SAMPLES_VALID / N_FAKE_STRATEGIES)\n",
    "N_USERS = purchases_df[\"customer_id\"].nunique()\n",
    "N_ITEMS = len(embedding)\n",
    "print(f\">> N_USERS = {N_USERS} | N_ITEMS = {N_ITEMS}\")\n",
    "\n",
    "\n",
    "# Actual sampling section\n",
    "print(\"\\nCreating samples using custom strategies\")\n",
    "strategy_handler = StrategyHandler(\n",
    "    vissimhandler, hybrid_scorer,\n",
    "    clustId2artIndexes, cluster_by_idx,\n",
    "    artistId2artworkIndexes, artist_by_idx,\n",
    "    threshold=FINE_GRAINED_THRESHOLD,\n",
    "    confidence_margin=CONFIDENCE_MARGIN,\n",
    ")\n",
    "\n",
    "print(\">> Strategy #1: Given real profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_1 = strategy_handler.strategy_1(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_1) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_1 = strategy_handler.strategy_1(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_1) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #1 Training samples ({len(samples_train_1)}) and validation samples ({len(samples_valid_1)})\")\n",
    "\n",
    "print(\">> Strategy #2: Given fake profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_2 = strategy_handler.strategy_2(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_2) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_2 = strategy_handler.strategy_2(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_2) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #2: Training samples ({len(samples_train_2)}) and validation samples ({len(samples_valid_2)})\")\n",
    "\n",
    "print(\">> Strategy #3: Given real profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_3 = strategy_handler.strategy_3(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_3) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_3 = strategy_handler.strategy_3(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_3) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #3: Training samples ({len(samples_train_3)}) and validation samples ({len(samples_valid_3)})\")\n",
    "\n",
    "print(\">> Strategy #4: Given fake profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_4 = strategy_handler.strategy_4(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_4) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_4 = strategy_handler.strategy_4(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_4) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #4: Training samples ({len(samples_train_4)}) and validation samples ({len(samples_valid_4)})\")\n",
    "\n",
    "# Log out detected collisions\n",
    "print(f\">> Total hash collisions: {hashes_container.collisions}\")\n",
    "print(f\">> Total visual collisions: {vissimhandler.count}\")\n",
    "\n",
    "\n",
    "# Merge triples into a single list\n",
    "print(\"\\nMerging strategies samples into a single list\")\n",
    "TRAINING_DATA = [samples_train_1, samples_train_2, samples_train_3, samples_train_4]\n",
    "for i, samples in enumerate(TRAINING_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "TRAINING_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in TRAINING_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Training samples: {len(TRAINING_DATA)}\")\n",
    "# Merge strategies samples\n",
    "VALIDATION_DATA = [samples_valid_1, samples_valid_2, samples_valid_3, samples_valid_4]\n",
    "for i, samples in enumerate(VALIDATION_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "VALIDATION_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in VALIDATION_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Validation samples: {len(VALIDATION_DATA)}\")\n",
    "\n",
    "\n",
    "# Search for duplicated hashes\n",
    "print(f\"\\nNaive triples validation and looking for duplicates...\")\n",
    "validation_hash_check = HashesContainer()\n",
    "all_samples = [\n",
    "    triple\n",
    "    for subset in (TRAINING_DATA, VALIDATION_DATA)\n",
    "    for triple in subset\n",
    "]\n",
    "customer_ids = purchases_df[\"customer_id\"].unique()\n",
    "user_data = dict()\n",
    "for triple in tqdm(all_samples, desc=\"Naive validation\"):\n",
    "    profile, pi, ni, ui = triple\n",
    "    assert validation_hash_check.enroll(pre_hash((profile, pi, ni)))\n",
    "    assert 0 <= pi < N_ITEMS\n",
    "    assert 0 <= ni < N_ITEMS\n",
    "    assert pi != ni\n",
    "    assert not vissimhandler.same(pi, ni)\n",
    "    if ui == -1:\n",
    "        continue\n",
    "    assert ui in customer_ids\n",
    "    if not ui in user_data:\n",
    "        user = purchases_df[purchases_df[\"customer_id\"] == ui]\n",
    "        user_data[ui] = set(np.concatenate(user[\"artwork_id\"].values))\n",
    "    user_artworks = user_data[ui]\n",
    "    assert all(i in user_artworks for i in profile)\n",
    "    spi = hybrid_scorer.get_score(ui, user_artworks, pi)\n",
    "    sni = hybrid_scorer.get_score(ui, user_artworks, ni)\n",
    "    assert spi > sni\n",
    "print(\">> No duped hashes found\")\n",
    "\n",
    "\n",
    "print(\"\\nCreating output files (train and valid)...\")\n",
    "# Training dataframe\n",
    "df_train = pd.DataFrame(TRAINING_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_train = df_train.drop(\"ui\", axis=1)\n",
    "df_train[\"profile\"] = df_train[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving training samples ({OUTPUT_TRAIN_PATH})\")\n",
    "df_train.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "\n",
    "# Validation dataframe\n",
    "df_validation = pd.DataFrame(VALIDATION_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_validation = df_validation.drop(\"ui\", axis=1)\n",
    "df_validation[\"profile\"] = df_validation[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving validation samples in ({OUTPUT_VALID_PATH})\")\n",
    "df_validation.to_csv(OUTPUT_VALID_PATH, index=False)\n",
    "\n",
    "\n",
    "print(\"\\nCreating output files (evaluation)...\")\n",
    "# Prepare existing dataframes\n",
    "inventory_df = inventory_df.rename(columns={\n",
    "    \"cluster_id\": \"visual_cluster_id\",\n",
    "})\n",
    "purchases_df = purchases_df.rename(columns={\n",
    "    \"artwork_id\": \"shopping_cart\",\n",
    "    \"customer_id\": \"user_id\",\n",
    "})\n",
    "purchases_df = purchases_df.drop([\"n_baskets\", \"n_items\", \"evaluation\"], axis=1)\n",
    "# Add event columns\n",
    "inventory_df[\"event\"] = \"inventory\"\n",
    "purchases_df[\"event\"] = \"purchase\"\n",
    "evaluation_df[\"event\"] = \"evaluation\"\n",
    "\n",
    "# Evaluation dataframe\n",
    "df_evaluation = pd.merge(inventory_df, purchases_df, on=\"timestamp\", how=\"outer\")\n",
    "df_evaluation = pd.merge(df_evaluation, evaluation_df, on=\"timestamp\", how=\"outer\")\n",
    "# Merge event columns\n",
    "df_evaluation[\"event\"] = df_evaluation[\"event\"].fillna(df_evaluation[\"event_x\"])\n",
    "df_evaluation[\"event\"] = df_evaluation[\"event\"].fillna(df_evaluation[\"event_y\"])\n",
    "df_evaluation = df_evaluation.drop([\"event_x\", \"event_y\"], axis=1)\n",
    "# Merge user_id columns\n",
    "df_evaluation[\"user_id\"] = df_evaluation[\"user_id_x\"].fillna(df_evaluation[\"user_id_y\"])\n",
    "df_evaluation = df_evaluation.drop([\"user_id_x\", \"user_id_y\"], axis=1)\n",
    "# Use timestamp to set index\n",
    "df_evaluation = df_evaluation.sort_values(by=[\"timestamp\"])\n",
    "df_evaluation = df_evaluation.reset_index(drop=True)\n",
    "# Move timestamp and event to first columns\n",
    "df_evaluation.insert(0, \"event\", df_evaluation.pop(\"event\"))\n",
    "df_evaluation.insert(0, \"timestamp\", df_evaluation.pop(\"timestamp\"))\n",
    "print(f\">> Saving evaluation data in ({OUTPUT_VALID_PATH})\")\n",
    "df_evaluation.to_csv(OUTPUT_EVAL_PATH, index=False)\n",
    "\n",
    "\n",
    "# Finished\n",
    "print(\"\\nDone\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.3",
   "language": "python",
   "name": "3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
