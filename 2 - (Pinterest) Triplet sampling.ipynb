{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.data.pinterest import (\n",
    "    get_interactions_dataframe, mark_evaluation_rows,\n",
    "    get_holdout,\n",
    ")\n",
    "from utils.hashing import pre_hash, HashesContainer\n",
    "from utils.sampling import StrategyHandler\n",
    "from utils.similarity import HybridScorer, VisualSimilarityHandler\n",
    "\n",
    "\n",
    "# Mode\n",
    "# Use 'MODE_PROFILE = True' for CuratorNet-like training \n",
    "# Use 'MODE_PROFILE = False' for VBPR-like training\n",
    "MODE_PROFILE = True\n",
    "MODE_PROFILE = \"profile\" if MODE_PROFILE else \"user\"\n",
    "\n",
    "# Parameters\n",
    "RNG_SEED = 0\n",
    "EMBEDDING_FN = os.path.join(\"data\", \"Pinterest\", \"pinterest_embedding.npy\")\n",
    "PCA_COMPONENTS = 200\n",
    "CLUSTERING_RNG = None\n",
    "CLUSTERING_N_CLUSTERS = 100\n",
    "CLUSTERING_N_TIMES = 5  # 20\n",
    "CLUSTERING_N_INIT = 1  # 8\n",
    "INTERACTIONS_PATH = os.path.join(\"data\", \"Pinterest\", \"pinterest.csv\")\n",
    "OUTPUT_TRAIN_PATH = os.path.join(\"data\", \"Pinterest\", f\"{MODE_PROFILE}-train.csv\")\n",
    "OUTPUT_VALID_PATH = os.path.join(\"data\", \"Pinterest\", f\"{MODE_PROFILE}-validation.csv\")\n",
    "OUTPUT_EVAL_PATH = os.path.join(\"data\", \"Pinterest\", f\"{MODE_PROFILE}-evaluation.csv\")\n",
    "\n",
    "# Parameters (sampling)\n",
    "ARTIST_BOOST = 0.2\n",
    "CONFIDENCE_MARGIN = 0.18\n",
    "FINE_GRAINED_THRESHOLD = 0.7\n",
    "FAKE_COEF = 0. if MODE_PROFILE is True else 0.\n",
    "assert all(\n",
    "    0. <= var <= 1.\n",
    "    for var in [ARTIST_BOOST, CONFIDENCE_MARGIN, FINE_GRAINED_THRESHOLD, FAKE_COEF]\n",
    ")\n",
    "TOTAL_SAMPLES_TRAIN = 3_000_000  # 10_000_000\n",
    "TOTAL_SAMPLES_VALID = 150_000  # 500_000\n",
    "\n",
    "# Parameters (checkpoints)\n",
    "LOAD_PCA_EMBEDDING = True\n",
    "PCA_EMBEDDING_FN = os.path.join(\"data\", \"Pinterest\", \"pinterest_pca.npy\")\n",
    "LOAD_PCA_LABELS = True\n",
    "PCA_LABELS_FN = os.path.join(\"data\", \"Pinterest\", \"pinterest_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# ~26 min\n",
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed...\")\n",
    "    random.seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n",
    "\n",
    "\n",
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_FN})\")\n",
    "embedding = np.load(EMBEDDING_FN, allow_pickle=True)\n",
    "\n",
    "\n",
    "# Extract features and id2index\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features = list()\n",
    "id2index = dict()\n",
    "for i, (_id, vector_embedding) in enumerate(embedding):\n",
    "    _id = str(_id)\n",
    "    if _id not in id2index:\n",
    "        id2index[_id] = len(features)\n",
    "        features.append(vector_embedding)\n",
    "features = np.asarray(features)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "# Release some memory\n",
    "del embedding\n",
    "\n",
    "\n",
    "if LOAD_PCA_EMBEDDING:\n",
    "    # Loading visual clusters\n",
    "    print(\"\\nLoading visual clusters: After scaling (0) and PCA (1)\")\n",
    "    features = np.load(PCA_EMBEDDING_FN, allow_pickle=True)\n",
    "else:\n",
    "    # Creating visual clusters (~58 min)\n",
    "    print(\"\\nCreating visual clusters: 0. z-score normalization of embedding\")\n",
    "    # features = StandardScaler().fit_transform(features)\n",
    "    features = StandardScaler().partial_fit(features).transform(features)\n",
    "    print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "    print(\"\\nCreating visual clusters: 1. Conduct (incremental) PCA to reduce dimension\")\n",
    "    features = IncrementalPCA(n_components=PCA_COMPONENTS).fit_transform(features)\n",
    "    np.save(PCA_EMBEDDING_FN, features, allow_pickle=True)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "\n",
    "if LOAD_PCA_LABELS:\n",
    "    # Loading cluster labels\n",
    "    print(\"\\nLoading visual clusters: After KMeans (2)\")\n",
    "    clusterer_labels = np.load(PCA_LABELS_FN, allow_pickle=True)\n",
    "    best_score = silhouette_score(features, clusterer_labels, sample_size=40_000)\n",
    "else:\n",
    "    # Creating cluster labels (~17 min)\n",
    "    print(\"\\nCreating visual clusters: 2. Perform k-means clustering\")\n",
    "    best_score = float(\"-inf\")\n",
    "    best_clusterer = None\n",
    "    for i in range(CLUSTERING_N_TIMES):\n",
    "        clusterer = MiniBatchKMeans(\n",
    "            n_clusters=CLUSTERING_N_CLUSTERS,\n",
    "            batch_size=2000,\n",
    "            max_iter=2000,\n",
    "            n_init=CLUSTERING_N_INIT,\n",
    "            random_state=CLUSTERING_RNG,\n",
    "            # verbose=1, ###\n",
    "        ).fit(features)\n",
    "        score = silhouette_score(features, clusterer.labels_, sample_size=40_000)\n",
    "        if score > best_score:\n",
    "            best_clusterer = clusterer\n",
    "            best_score = score\n",
    "        if CLUSTERING_RNG is not None:\n",
    "            break\n",
    "        print((f\">> Silhouette score ({i + 1:02}/{CLUSTERING_N_TIMES}): \"\n",
    "               f\"{score:.4f} (Best: {best_score:.4f})\"), flush=True, end=\"\\r\")\n",
    "    clusterer_labels = best_clusterer.labels_\n",
    "    np.save(PCA_LABELS_FN, clusterer_labels, allow_pickle=True)\n",
    "print(f\">> Best Silhouette score: {best_score}\")\n",
    "\n",
    "\n",
    "# Load interactions CSVs\n",
    "print(f\"\\nLoading interactions from files...\")\n",
    "interactions_df = get_interactions_dataframe(\n",
    "    INTERACTIONS_PATH,\n",
    "    display_stats=True,\n",
    ")\n",
    "\n",
    "# Apply id2index, to work with indexes only\n",
    "interactions_df[\"item_id\"] = interactions_df[\"item_id\"].map(id2index)\n",
    "print(f\">> Mapping applied ({interactions_df['item_id'].isna().sum()} missing values)\")\n",
    "\n",
    "# Select available images only\n",
    "interactions_df = interactions_df[interactions_df[\"item_id\"].notnull()]\n",
    "# Transform indexes to int\n",
    "interactions_df[\"item_id\"] = interactions_df[\"item_id\"].astype(int)\n",
    "\n",
    "# Store mapping from user id to index (0-index, no skipping)\n",
    "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
    "new_user_ids = np.argsort(unique_user_ids)\n",
    "user_id_map = dict(zip(\n",
    "    unique_user_ids,\n",
    "    new_user_ids,\n",
    "))\n",
    "\n",
    "# Mark interactions used for evaluation procedure\n",
    "interactions_df = mark_evaluation_rows(interactions_df, threshold=10)\n",
    "# Check if new column exists and has boolean dtype\n",
    "assert interactions_df[\"evaluation\"].dtype.name == \"bool\"\n",
    "print(f\">> Interactions: {interactions_df.shape}\")\n",
    "\n",
    "# Store index for sorting\n",
    "interactions_df[\"index\"] = interactions_df.index\n",
    "# Split interactions data according to evaluation column\n",
    "evaluation_df, interactions_df = get_holdout(interactions_df)\n",
    "assert not interactions_df.empty\n",
    "assert not evaluation_df.empty\n",
    "print(f\">> Evaluation: {evaluation_df.shape} | Interactions: {interactions_df.shape}\")\n",
    "\n",
    "\n",
    "# Create helper mapping from idx to data\n",
    "print(\"\\nCreating mappings from index to data\")\n",
    "artist_by_idx = np.full((features.shape[0],), -1)\n",
    "cluster_by_idx = clusterer_labels\n",
    "\n",
    "\n",
    "# Create helper mapping from data to idxs\n",
    "print(\"\\nCreating mappings from data to index\")\n",
    "artistId2artworkIndexes = {-1: interactions_df[\"item_id\"].unique()}\n",
    "clustId2artIndexes = dict()\n",
    "for i, cluster in enumerate(cluster_by_idx):\n",
    "    if cluster not in clustId2artIndexes:\n",
    "        clustId2artIndexes[cluster] = list()\n",
    "    clustId2artIndexes[cluster].append(i)\n",
    "\n",
    "\n",
    "print(\"\\nCreating helpers instances...\")\n",
    "# Creating hashes container for duplicates detection\n",
    "hashes_container = HashesContainer()\n",
    "# Creating custom score helpers\n",
    "vissimhandler = VisualSimilarityHandler(clusterer_labels, features)\n",
    "hybrid_scorer = HybridScorer(vissimhandler, artist_by_idx, artist_boost=ARTIST_BOOST)\n",
    "\n",
    "\n",
    "# Sampling constants\n",
    "print(\"\\nCalculating important values...\")\n",
    "N_REAL_STRATEGIES = 2\n",
    "N_FAKE_STRATEGIES = 2\n",
    "print(f\">> There are {N_REAL_STRATEGIES} real strategies and {N_FAKE_STRATEGIES} fake strategies\")\n",
    "N_SAMPLES_PER_REAL_STRAT_TRAIN = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_TRAIN / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_REAL_STRAT_VALID = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_VALID / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_TRAIN = ceil(FAKE_COEF * TOTAL_SAMPLES_TRAIN / N_FAKE_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_VALID = ceil(FAKE_COEF * TOTAL_SAMPLES_VALID / N_FAKE_STRATEGIES)\n",
    "N_USERS = interactions_df[\"user_id\"].nunique()\n",
    "N_ITEMS = len(features)\n",
    "print(f\">> N_USERS = {N_USERS} | N_ITEMS = {N_ITEMS}\")\n",
    "\n",
    "\n",
    "# Actual sampling section\n",
    "print(\"\\nCreating samples using custom strategies\")\n",
    "strategy_handler = StrategyHandler(\n",
    "    vissimhandler, hybrid_scorer,\n",
    "    clustId2artIndexes, cluster_by_idx,\n",
    "    artistId2artworkIndexes, artist_by_idx,\n",
    "    threshold=FINE_GRAINED_THRESHOLD,\n",
    "    confidence_margin=CONFIDENCE_MARGIN,\n",
    "    user_as_items=MODE_PROFILE,\n",
    ")\n",
    "\n",
    "print(\">> Strategy #1: Given real profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_1 = strategy_handler.strategy_1(\n",
    "    interactions_df.copy(),  # interactions_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_1) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_1 = strategy_handler.strategy_1(\n",
    "    interactions_df.copy(),  # interactions_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_1) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #1 Training samples ({len(samples_train_1)}) and validation samples ({len(samples_valid_1)})\")\n",
    "\n",
    "print(\">> Strategy #2: Given fake profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_2 = strategy_handler.strategy_2(\n",
    "    features,  # features\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_2) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_2 = strategy_handler.strategy_2(\n",
    "    features,  # features\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_2) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #2: Training samples ({len(samples_train_2)}) and validation samples ({len(samples_valid_2)})\")\n",
    "\n",
    "print(\">> Strategy #3: Given real profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_3 = strategy_handler.strategy_3(\n",
    "    interactions_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_3) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_3 = strategy_handler.strategy_3(\n",
    "    interactions_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_3) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #3: Training samples ({len(samples_train_3)}) and validation samples ({len(samples_valid_3)})\")\n",
    "\n",
    "print(\">> Strategy #4: Given fake profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_4 = strategy_handler.strategy_4(\n",
    "    features,  # features\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_4) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_4 = strategy_handler.strategy_4(\n",
    "    features,  # features\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_4) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #4: Training samples ({len(samples_train_4)}) and validation samples ({len(samples_valid_4)})\")\n",
    "\n",
    "# Log out detected collisions\n",
    "print(f\">> Total hash collisions: {hashes_container.collisions}\")\n",
    "print(f\">> Total visual collisions: {vissimhandler.count}\")\n",
    "\n",
    "\n",
    "# Merge triples into a single list\n",
    "print(\"\\nMerging strategies samples into a single list\")\n",
    "TRAINING_DATA = [samples_train_1, samples_train_2, samples_train_3, samples_train_4]\n",
    "for i, samples in enumerate(TRAINING_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "TRAINING_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in TRAINING_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Training samples: {len(TRAINING_DATA)}\")\n",
    "# Merge strategies samples\n",
    "VALIDATION_DATA = [samples_valid_1, samples_valid_2, samples_valid_3, samples_valid_4]\n",
    "for i, samples in enumerate(VALIDATION_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "VALIDATION_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in VALIDATION_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Validation samples: {len(VALIDATION_DATA)}\")\n",
    "\n",
    "\n",
    "# Search for duplicated hashes\n",
    "print(f\"\\nNaive triples validation and looking for duplicates...\")\n",
    "validation_hash_check = HashesContainer()\n",
    "all_samples = [\n",
    "    triple\n",
    "    for subset in (TRAINING_DATA, VALIDATION_DATA)\n",
    "    for triple in subset\n",
    "]\n",
    "user_ids = interactions_df[\"user_id\"].unique()\n",
    "user_data = dict()\n",
    "for triple in tqdm(all_samples, desc=\"Naive validation\"):\n",
    "    profile, pi, ni, ui = triple\n",
    "    if MODE_PROFILE:\n",
    "        assert validation_hash_check.enroll(pre_hash((profile, pi, ni)))\n",
    "    else:\n",
    "        assert validation_hash_check.enroll(pre_hash((ui, pi, ni), contains_iter=False))\n",
    "    assert 0 <= pi < N_ITEMS\n",
    "    assert 0 <= ni < N_ITEMS\n",
    "    assert pi != ni\n",
    "    assert not vissimhandler.same(pi, ni)\n",
    "    if ui == -1:\n",
    "        continue\n",
    "    assert ui in user_ids\n",
    "    if not ui in user_data:\n",
    "        user = interactions_df[interactions_df[\"user_id\"] == ui]\n",
    "        user_data[ui] = set(np.hstack(user[\"item_id\"].values))\n",
    "    user_artworks = user_data[ui]\n",
    "    assert all(i in user_artworks for i in profile)\n",
    "    spi = hybrid_scorer.get_score(ui, user_artworks, pi)\n",
    "    sni = hybrid_scorer.get_score(ui, user_artworks, ni)\n",
    "    assert spi > sni\n",
    "print(\">> No duped hashes found\")\n",
    "\n",
    "\n",
    "print(\"\\nCreating output files (train and valid)...\")\n",
    "# Training dataframe\n",
    "df_train = pd.DataFrame(TRAINING_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_train[\"ui\"] = df_train[\"ui\"].map(user_id_map)\n",
    "df_train[\"profile\"] = df_train[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving training samples ({OUTPUT_TRAIN_PATH})\")\n",
    "df_train.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "\n",
    "# Validation dataframe\n",
    "df_validation = pd.DataFrame(VALIDATION_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_validation[\"ui\"] = df_validation[\"ui\"].map(user_id_map)\n",
    "df_validation[\"profile\"] = df_validation[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving validation samples in ({OUTPUT_VALID_PATH})\")\n",
    "df_validation.to_csv(OUTPUT_VALID_PATH, index=False)\n",
    "\n",
    "\n",
    "print(\"\\nCreating output files (evaluation)...\")\n",
    "# Add event columns\n",
    "interactions_df[\"event\"] = \"interaction\"\n",
    "evaluation_df[\"event\"] = \"evaluation\"\n",
    "\n",
    "# Evaluation dataframe\n",
    "df_evaluation = interactions_df.append(evaluation_df)\n",
    "# Map user_id columns\n",
    "df_evaluation[\"user_id\"] = df_evaluation[\"user_id\"].map(user_id_map)\n",
    "# Use old index to set new index\n",
    "df_evaluation = df_evaluation.sort_values(by=[\"index\"])\n",
    "df_evaluation = df_evaluation.reset_index(drop=True)\n",
    "# Move timestamp and event to first columns\n",
    "df_evaluation.insert(0, \"event\", df_evaluation.pop(\"event\"))\n",
    "df_evaluation = df_evaluation.drop([\"index\"], axis=1)\n",
    "print(f\">> Saving evaluation data in ({OUTPUT_VALID_PATH})\")\n",
    "df_evaluation.to_csv(OUTPUT_EVAL_PATH, index=False)\n",
    "\n",
    "\n",
    "# Finished\n",
    "print(\"\\nDone\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.3",
   "language": "python",
   "name": "3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
