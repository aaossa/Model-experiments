{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.data.ugallery import (\n",
    "    get_transactions_dataframes, add_aggregation_columns,\n",
    "    mark_evaluation_rows, get_holdout, map_ids_to_indexes,\n",
    ")\n",
    "from utils.hashing import pre_hash, HashesContainer\n",
    "from utils.sampling import StrategyHandler\n",
    "from utils.similarity import HybridScorer, VisualSimilarityHandler\n",
    "\n",
    "\n",
    "# Mode\n",
    "# Use 'MODE_PROFILE = True' for CuratorNet-like training \n",
    "# Use 'MODE_PROFILE = False' for VBPR-like training\n",
    "MODE_PROFILE = True\n",
    "MODE_PROFILE = \"profile\" if MODE_PROFILE else \"user\"\n",
    "\n",
    "# Parameters\n",
    "RNG_SEED = 0\n",
    "EMBEDDING_FN = os.path.join(\"data\", \"UGallery\", \"ugallery_embedding.npy\")\n",
    "PCA_COMPONENTS = 200\n",
    "CLUSTERING_RNG = None\n",
    "CLUSTERING_N_CLUSTERS = 100\n",
    "CLUSTERING_N_TIMES = 5  # 20\n",
    "CLUSTERING_N_INIT = 1  # 8\n",
    "INVENTORY_PATH = os.path.join(\"data\", \"UGallery\", \"valid_artworks.csv\")\n",
    "PURCHASES_PATH = os.path.join(\"data\", \"UGallery\", \"valid_sales.csv\")\n",
    "OUTPUT_TRAIN_PATH = os.path.join(\"data\", \"UGallery\", f\"{MODE_PROFILE}-train.csv\")\n",
    "OUTPUT_VALID_PATH = os.path.join(\"data\", \"UGallery\", f\"{MODE_PROFILE}-validation.csv\")\n",
    "OUTPUT_EVAL_PATH = os.path.join(\"data\", \"UGallery\", f\"{MODE_PROFILE}-evaluation.csv\")\n",
    "\n",
    "# Parameters (sampling)\n",
    "ARTIST_BOOST = 0.2\n",
    "CONFIDENCE_MARGIN = 0.18\n",
    "FINE_GRAINED_THRESHOLD = 0.7\n",
    "FAKE_COEF = 0. if MODE_PROFILE is True else 0.\n",
    "assert all(\n",
    "    0. <= var <= 1.\n",
    "    for var in [ARTIST_BOOST, CONFIDENCE_MARGIN, FINE_GRAINED_THRESHOLD, FAKE_COEF]\n",
    ")\n",
    "PROFILE_SIZE = 10\n",
    "TOTAL_SAMPLES_TRAIN = 10_000_000\n",
    "TOTAL_SAMPLES_VALID = 500_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# ~10 min\n",
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed...\")\n",
    "    random.seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n",
    "\n",
    "\n",
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_FN})\")\n",
    "embedding = np.load(EMBEDDING_FN, allow_pickle=True)\n",
    "\n",
    "\n",
    "# Extract features and id2index\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features = np.zeros(shape=(embedding.shape[0], embedding[0, 1].shape[0]))\n",
    "id2index = dict()\n",
    "for i, (_id, vector_embedding) in enumerate(embedding):\n",
    "    features[i] = vector_embedding\n",
    "    id2index[str(_id)] = i\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "\n",
    "# Creating visual clusters\n",
    "print(\"\\nCreating visual clusters: 0. z-score normalization of embedding\")\n",
    "features = StandardScaler().fit_transform(features)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "print(\"\\nCreating visual clusters: 1. Conduct PCA to reduce dimension\")\n",
    "features = PCA(n_components=PCA_COMPONENTS).fit_transform(features)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\nCreating visual clusters: 2. Perform k-means clustering\")\n",
    "best_score = float(\"-inf\")\n",
    "best_clusterer = None\n",
    "for i in range(CLUSTERING_N_TIMES):\n",
    "    clusterer = KMeans(\n",
    "        n_clusters=CLUSTERING_N_CLUSTERS,\n",
    "        max_iter=2000,\n",
    "        n_init=CLUSTERING_N_INIT,\n",
    "        random_state=CLUSTERING_RNG,\n",
    "    ).fit(features)\n",
    "    score = silhouette_score(features, clusterer.labels_)\n",
    "    if score > best_score:\n",
    "        best_clusterer = clusterer\n",
    "        best_score = score\n",
    "    if CLUSTERING_RNG is not None:\n",
    "        break\n",
    "    print((f\">> Silhouette score ({i + 1:02}/{CLUSTERING_N_TIMES}): \"\n",
    "           f\"{score:.4f} (Best: {best_score:.4f})\"), flush=True, end=\"\\r\")\n",
    "print(f\">> Best Silhouette score: {best_score}\")\n",
    "\n",
    "\n",
    "# Load transactions CSVs\n",
    "print(f\"\\nLoading transactions from files...\")\n",
    "inventory_df, purchases_df = get_transactions_dataframes(\n",
    "    INVENTORY_PATH, PURCHASES_PATH,\n",
    "    display_stats=False,\n",
    ")\n",
    "\n",
    "# Store mapping from user id to index (0-index, no skipping)\n",
    "unique_user_ids = purchases_df[\"user_id\"].unique()\n",
    "new_user_ids = np.argsort(unique_user_ids)\n",
    "user_id_map = dict(zip(\n",
    "    unique_user_ids,\n",
    "    new_user_ids,\n",
    "))\n",
    "\n",
    "# Apply id2index, to work with indexes only\n",
    "inventory_df = map_ids_to_indexes(inventory_df, id2index)\n",
    "purchases_df = map_ids_to_indexes(purchases_df, id2index)\n",
    "print(\">> Mapping applied\")\n",
    "\n",
    "\n",
    "# Catch consumable and non consumable items\n",
    "sold_once = []\n",
    "sold_more_than_once = []\n",
    "for purchase in purchases_df[\"item_id\"]:\n",
    "    for item in purchase:\n",
    "        if item not in sold_once:\n",
    "            sold_once.append(item)\n",
    "        else:\n",
    "            if item not in sold_more_than_once:\n",
    "                sold_more_than_once.append(item)\n",
    "print(f\"Unique items (consumable): {len(sold_once)}\")\n",
    "print(f\"Duped items (non consumable): {len(sold_more_than_once)}\")\n",
    "\n",
    "# Simulate inventory consistency\n",
    "sim_inventory = pd.merge(inventory_df[[\"timestamp\", \"item_id\"]], purchases_df[[\"timestamp\", \"item_id\"]], on=\"timestamp\", how=\"outer\")\n",
    "sim_inventory = sim_inventory.sort_values(\"timestamp\")\n",
    "inventory = []\n",
    "for timestamp, group in sim_inventory.groupby(\"timestamp\"):\n",
    "    additions = group[\"item_id_x\"]\n",
    "    removals = group[\"item_id_y\"]\n",
    "    assert bool(additions.isnull().values.any()) is not bool(removals.isnull().values.any())\n",
    "    if bool(removals.isnull().values.any()):\n",
    "        # Addition of item\n",
    "        for item in additions:\n",
    "            inventory.append(item)\n",
    "    else:\n",
    "        # Removal of items\n",
    "        for sale in removals:\n",
    "            for item in sale:\n",
    "                if item not in sold_more_than_once:\n",
    "                    assert item in inventory, f\"{item}\"\n",
    "                    inventory.remove(item)\n",
    "\n",
    "# Add consumable condition to inventory dataframe\n",
    "inventory_df[\"consumable\"] = True\n",
    "inventory_df.loc[inventory_df[\"item_id\"].isin(sold_more_than_once), \"consumable\"] = False\n",
    "assert len(inventory_df[inventory_df[\"consumable\"]]) == len(inventory_df) - len(sold_more_than_once)\n",
    "assert len(inventory_df[~inventory_df[\"consumable\"]]) == len(sold_more_than_once)\n",
    "\n",
    "\n",
    "# Calculating number of baskets and size of each basket for purchases\n",
    "purchases_df = add_aggregation_columns(purchases_df)\n",
    "# Check if new values are reasonable\n",
    "assert all(purchases_df[\"n_baskets\"] > 0)\n",
    "assert all(purchases_df[\"n_items\"] > 0)\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Mark purchases used for evaluation procedure\n",
    "purchases_df = mark_evaluation_rows(purchases_df)\n",
    "# Check if new column exists and has boolean dtype\n",
    "assert purchases_df[\"evaluation\"].dtype.name == \"bool\"\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Split purchases data according to evaluation column\n",
    "evaluation_df, purchases_df = get_holdout(purchases_df)\n",
    "assert not purchases_df.empty\n",
    "assert not evaluation_df.empty\n",
    "print(f\">> Evaluation: {evaluation_df.shape} | Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Recalculate number of baskets and size of each basket for purchases\n",
    "purchases_df = add_aggregation_columns(purchases_df)\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Add cluster id information\n",
    "inventory_df[\"cluster_id\"] = inventory_df[\"item_id\"].apply(\n",
    "    lambda idx: best_clusterer.labels_[idx],\n",
    ")\n",
    "print(f\">> Inventory: {inventory_df.shape}\")\n",
    "\n",
    "\n",
    "# Create helper mapping from idx to data\n",
    "print(\"\\nCreating mappings from index to data\")\n",
    "artist_by_idx = np.full((features.shape[0],), -1)\n",
    "for item_id, artist_id in inventory_df.set_index(\"item_id\").to_dict()[\"artist_id\"].items():\n",
    "    artist_by_idx[item_id] = artist_id\n",
    "cluster_by_idx = best_clusterer.labels_\n",
    "\n",
    "\n",
    "# Create helper mapping from data to idxs\n",
    "print(\"\\nCreating mappings from data to index\")\n",
    "artistId2artworkIndexes = inventory_df.groupby(\"artist_id\")[\"item_id\"].apply(list).to_dict()\n",
    "clustId2artIndexes = dict()\n",
    "for i, cluster in enumerate(cluster_by_idx):\n",
    "    if cluster not in clustId2artIndexes:\n",
    "        clustId2artIndexes[cluster] = list()\n",
    "    clustId2artIndexes[cluster].append(i)\n",
    "\n",
    "\n",
    "print(\"\\nCreating helpers instances...\")\n",
    "# Creating hashes container for duplicates detection\n",
    "hashes_container = HashesContainer()\n",
    "# Creating custom score helpers\n",
    "vissimhandler = VisualSimilarityHandler(best_clusterer.labels_, features)\n",
    "hybrid_scorer = HybridScorer(vissimhandler, artist_by_idx, artist_boost=ARTIST_BOOST)\n",
    "\n",
    "\n",
    "# Sampling constants\n",
    "print(\"\\nCalculating important values...\")\n",
    "N_REAL_STRATEGIES = 2\n",
    "N_FAKE_STRATEGIES = 2\n",
    "print(f\">> There are {N_REAL_STRATEGIES} real strategies and {N_FAKE_STRATEGIES} fake strategies\")\n",
    "N_SAMPLES_PER_REAL_STRAT_TRAIN = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_TRAIN / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_REAL_STRAT_VALID = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_VALID / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_TRAIN = ceil(FAKE_COEF * TOTAL_SAMPLES_TRAIN / N_FAKE_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_VALID = ceil(FAKE_COEF * TOTAL_SAMPLES_VALID / N_FAKE_STRATEGIES)\n",
    "N_USERS = purchases_df[\"user_id\"].nunique()\n",
    "N_ITEMS = len(embedding)\n",
    "print(f\">> N_USERS = {N_USERS} | N_ITEMS = {N_ITEMS}\")\n",
    "\n",
    "\n",
    "# Actual sampling section\n",
    "print(\"\\nCreating samples using custom strategies\")\n",
    "strategy_handler = StrategyHandler(\n",
    "    vissimhandler, hybrid_scorer,\n",
    "    clustId2artIndexes, cluster_by_idx,\n",
    "    artistId2artworkIndexes, artist_by_idx,\n",
    "    threshold=FINE_GRAINED_THRESHOLD,\n",
    "    confidence_margin=CONFIDENCE_MARGIN,\n",
    "    user_as_items=MODE_PROFILE,\n",
    "    max_profile_size=PROFILE_SIZE,\n",
    ")\n",
    "\n",
    "print(\">> Strategy #1: Given real profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_1 = strategy_handler.strategy_1(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_1) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_1 = strategy_handler.strategy_1(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_1) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #1 Training samples ({len(samples_train_1)}) and validation samples ({len(samples_valid_1)})\")\n",
    "\n",
    "print(\">> Strategy #2: Given fake profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_2 = strategy_handler.strategy_2(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_2) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_2 = strategy_handler.strategy_2(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_2) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #2: Training samples ({len(samples_train_2)}) and validation samples ({len(samples_valid_2)})\")\n",
    "\n",
    "print(\">> Strategy #3: Given real profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_3 = strategy_handler.strategy_3(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_3) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_3 = strategy_handler.strategy_3(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_3) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #3: Training samples ({len(samples_train_3)}) and validation samples ({len(samples_valid_3)})\")\n",
    "\n",
    "print(\">> Strategy #4: Given fake profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_4 = strategy_handler.strategy_4(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_4) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_4 = strategy_handler.strategy_4(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_4) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #4: Training samples ({len(samples_train_4)}) and validation samples ({len(samples_valid_4)})\")\n",
    "\n",
    "# Log out detected collisions\n",
    "print(f\">> Total hash collisions: {hashes_container.collisions}\")\n",
    "print(f\">> Total visual collisions: {vissimhandler.count}\")\n",
    "\n",
    "\n",
    "# Merge triples into a single list\n",
    "print(\"\\nMerging strategies samples into a single list\")\n",
    "TRAINING_DATA = [samples_train_1, samples_train_2, samples_train_3, samples_train_4]\n",
    "for i, samples in enumerate(TRAINING_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "TRAINING_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in TRAINING_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Training samples: {len(TRAINING_DATA)}\")\n",
    "# Merge strategies samples\n",
    "VALIDATION_DATA = [samples_valid_1, samples_valid_2, samples_valid_3, samples_valid_4]\n",
    "for i, samples in enumerate(VALIDATION_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "VALIDATION_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in VALIDATION_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Validation samples: {len(VALIDATION_DATA)}\")\n",
    "\n",
    "\n",
    "# Search for duplicated hashes\n",
    "print(f\"\\nNaive triples validation and looking for duplicates...\")\n",
    "validation_hash_check = HashesContainer()\n",
    "all_samples = [\n",
    "    triple\n",
    "    for subset in (TRAINING_DATA, VALIDATION_DATA)\n",
    "    for triple in subset\n",
    "]\n",
    "user_ids = purchases_df[\"user_id\"].unique()\n",
    "user_data = dict()\n",
    "for triple in tqdm(all_samples, desc=\"Naive validation\"):\n",
    "    profile, pi, ni, ui = triple\n",
    "    if MODE_PROFILE:\n",
    "        assert validation_hash_check.enroll(pre_hash((profile, pi, ni)))\n",
    "    else:\n",
    "        assert validation_hash_check.enroll(pre_hash((ui, pi, ni), contains_iter=False))\n",
    "    assert 0 <= pi < N_ITEMS\n",
    "    assert 0 <= ni < N_ITEMS\n",
    "    assert pi != ni\n",
    "    assert not vissimhandler.same(pi, ni)\n",
    "    if ui == -1:\n",
    "        continue\n",
    "    assert ui in user_ids\n",
    "    if not ui in user_data:\n",
    "        user = purchases_df[purchases_df[\"user_id\"] == ui]\n",
    "        user_data[ui] = set(np.concatenate(user[\"item_id\"].values))\n",
    "    user_artworks = user_data[ui]\n",
    "    assert all(i in user_artworks for i in profile)\n",
    "    spi = hybrid_scorer.get_score(ui, user_artworks, pi)\n",
    "    sni = hybrid_scorer.get_score(ui, user_artworks, ni)\n",
    "    assert spi > sni\n",
    "print(\">> No duped hashes found\")\n",
    "\n",
    "\n",
    "print(\"\\nCreating output files (train and valid)...\")\n",
    "# Training dataframe\n",
    "df_train = pd.DataFrame(TRAINING_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_train[\"ui\"] = df_train[\"ui\"].map(user_id_map)\n",
    "df_train[\"profile\"] = df_train[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving training samples ({OUTPUT_TRAIN_PATH})\")\n",
    "df_train.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "\n",
    "# Validation dataframe\n",
    "df_validation = pd.DataFrame(VALIDATION_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_validation[\"ui\"] = df_validation[\"ui\"].map(user_id_map)\n",
    "df_validation[\"profile\"] = df_validation[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving validation samples in ({OUTPUT_VALID_PATH})\")\n",
    "df_validation.to_csv(OUTPUT_VALID_PATH, index=False)\n",
    "\n",
    "\n",
    "print(\"\\nCreating output files (evaluation)...\")\n",
    "# Prepare existing dataframes\n",
    "inventory_df = inventory_df.rename(columns={\n",
    "    \"cluster_id\": \"visual_cluster_id\",\n",
    "})\n",
    "purchases_df = purchases_df.rename(columns={\n",
    "    \"item_id\": \"shopping_cart\",\n",
    "    # \"customer_id\": \"user_id\",\n",
    "})\n",
    "purchases_df = purchases_df.drop([\"n_baskets\", \"n_items\", \"evaluation\"], axis=1)\n",
    "# Add event columns\n",
    "inventory_df[\"event\"] = \"inventory\"\n",
    "purchases_df[\"event\"] = \"purchase\"\n",
    "evaluation_df[\"event\"] = \"evaluation\"\n",
    "\n",
    "# Evaluation dataframe\n",
    "df_evaluation = pd.merge(inventory_df, purchases_df, on=\"timestamp\", how=\"outer\")\n",
    "df_evaluation = pd.merge(df_evaluation, evaluation_df, on=\"timestamp\", how=\"outer\")\n",
    "# Merge event columns\n",
    "df_evaluation[\"event\"] = df_evaluation[\"event\"].fillna(df_evaluation[\"event_x\"])\n",
    "df_evaluation[\"event\"] = df_evaluation[\"event\"].fillna(df_evaluation[\"event_y\"])\n",
    "df_evaluation = df_evaluation.drop([\"event_x\", \"event_y\"], axis=1)\n",
    "# Merge user_id columns\n",
    "df_evaluation[\"user_id\"] = df_evaluation[\"user_id_x\"].fillna(df_evaluation[\"user_id_y\"])\n",
    "df_evaluation = df_evaluation.drop([\"user_id_x\", \"user_id_y\"], axis=1)\n",
    "df_evaluation[\"user_id\"] = df_evaluation[\"user_id\"].map(user_id_map)\n",
    "# Use timestamp to set index\n",
    "df_evaluation = df_evaluation.sort_values(by=[\"timestamp\"])\n",
    "df_evaluation = df_evaluation.reset_index(drop=True)\n",
    "# Move timestamp and event to first columns\n",
    "df_evaluation.insert(0, \"event\", df_evaluation.pop(\"event\"))\n",
    "df_evaluation.insert(0, \"timestamp\", df_evaluation.pop(\"timestamp\"))\n",
    "print(f\">> Saving evaluation data in ({OUTPUT_EVAL_PATH})\")\n",
    "df_evaluation.to_csv(OUTPUT_EVAL_PATH, index=False)\n",
    "\n",
    "\n",
    "# Finished\n",
    "print(\"\\nDone\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.5",
   "language": "python",
   "name": "3.8.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
