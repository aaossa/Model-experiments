{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from utils.ugallery.data import (\n",
    "    get_transactions_dataframes, add_aggregation_columns,\n",
    "    mark_evaluation_rows, get_holdout, map_ids_to_indexes,\n",
    ")\n",
    "from utils.ugallery.hashing import pre_hash, HashesContainer\n",
    "from utils.ugallery.sampling import StrategyHandler\n",
    "from utils.ugallery.similarity import HybridScorer, VisualSimilarityHandler\n",
    "\n",
    "\n",
    "# Parameters\n",
    "RNG_SEED = 0\n",
    "EMBEDDING_FN = os.path.join(\"ugallery_embedding.npy\")\n",
    "PCA_COMPONENTS = 200\n",
    "CLUSTERING_RNG = None\n",
    "CLUSTERING_N_CLUSTERS = 100\n",
    "CLUSTERING_N_TIMES = 5  # 20\n",
    "CLUSTERING_N_INIT = 1  # 8\n",
    "CLUSTERING_N_JOBS = os.cpu_count()  # 4 or 8\n",
    "INVENTORY_PATH = os.path.join(\"valid_artworks.csv\")\n",
    "PURCHASES_PATH = os.path.join(\"valid_sales.csv\")\n",
    "OUTPUT_TRAIN_PATH = os.path.join(\"train.csv\")\n",
    "OUTPUT_VALID_PATH = os.path.join(\"validation.csv\")\n",
    "OUTPUT_EVAL_PATH = os.path.join(\"evaluation.json\")\n",
    "\n",
    "# Parameters (sampling)\n",
    "ARTIST_BOOST = 0.2\n",
    "CONFIDENCE_MARGIN = 0.18\n",
    "FINE_GRAINED_THRESHOLD = 0.7\n",
    "FAKE_COEF = 0.\n",
    "assert all(\n",
    "    0. <= var <= 1.\n",
    "    for var in [ARTIST_BOOST, CONFIDENCE_MARGIN, FINE_GRAINED_THRESHOLD, FAKE_COEF]\n",
    ")\n",
    "TOTAL_SAMPLES_TRAIN = 10_000_000\n",
    "TOTAL_SAMPLES_VALID = 500_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed...\")\n",
    "    random.seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n",
    "\n",
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_FN})\")\n",
    "embedding = np.load(EMBEDDING_FN, allow_pickle=True)\n",
    "\n",
    "# Extract features and id2index\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features = np.zeros(shape=(embedding.shape[0], embedding[0, 1].shape[0]))\n",
    "id2index = dict()\n",
    "for i, (_id, vector_embedding) in enumerate(embedding):\n",
    "    features[i] = vector_embedding\n",
    "    id2index[str(_id)] = i\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "# Creating visual clusters\n",
    "print(\"\\nCreating visual clusters: 0. z-score normalization of embedding\")\n",
    "features = StandardScaler().fit_transform(features)\n",
    "print(f\">> Features shape: {features.shape}\") ###\n",
    "\n",
    "print(\"\\nCreating visual clusters: 1. Conduct PCA to reduce dimension\")\n",
    "features = PCA(n_components=PCA_COMPONENTS).fit_transform(features)\n",
    "print(f\">> Features shape: {features.shape}\") ###\n",
    "\n",
    "print(\"\\nCreating visual clusters: 2. Perform k-means clustering\")\n",
    "best_score = float(\"-inf\")\n",
    "best_clusterer = None\n",
    "for i in range(CLUSTERING_N_TIMES):\n",
    "    clusterer = KMeans(\n",
    "        n_clusters=CLUSTERING_N_CLUSTERS,\n",
    "        max_iter=2000,\n",
    "        n_init=CLUSTERING_N_INIT,\n",
    "        n_jobs=CLUSTERING_N_JOBS,\n",
    "        random_state=CLUSTERING_RNG,\n",
    "    ).fit(features)\n",
    "    score = silhouette_score(features, clusterer.labels_)\n",
    "    if score > best_score:\n",
    "        best_clusterer = clusterer\n",
    "        best_score = score\n",
    "    if CLUSTERING_RNG is not None:\n",
    "        break\n",
    "    print((f\">> Silhouette score ({i + 1:02}/{CLUSTERING_N_TIMES}): \"\n",
    "           f\"{score:.4f} (Best: {best_score:.4f})\"), flush=True, end=\"\\r\")\n",
    "print(f\">> Best Silhouette score: {best_score}\")\n",
    "\n",
    "# Load transactions CSVs\n",
    "print(f\"\\nLoading transactions from files...\")\n",
    "inventory_df, purchases_df = get_transactions_dataframes(\n",
    "    INVENTORY_PATH, PURCHASES_PATH,\n",
    "    display_stats=False,\n",
    ")\n",
    "# Check if every purchased artwork is present in inventory\n",
    "for artwork_id in purchases_df[\"artwork_id\"].sum():\n",
    "    assert artwork_id in inventory_df[\"artwork_id\"].values\n",
    "print(f\">> Inventory: {inventory_df.shape} | Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Calculating number of baskets and size of each basket for purchases\n",
    "purchases_df = add_aggregation_columns(purchases_df)\n",
    "# Check if new values are reasonable\n",
    "assert all(purchases_df[\"n_baskets\"] > 0)\n",
    "assert all(purchases_df[\"n_items\"] > 0)\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Mark purchases used for evaluation procedure\n",
    "purchases_df = mark_evaluation_rows(purchases_df)\n",
    "# Check if new column exists and has boolean dtype\n",
    "assert purchases_df[\"evaluation\"].dtype.name == \"bool\"\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Split purchases data according to evaluation column\n",
    "evaluation_df, purchases_df = get_holdout(purchases_df)\n",
    "assert not purchases_df.empty\n",
    "assert not evaluation_df.empty\n",
    "print(f\">> Evaluation: {evaluation_df.shape} | Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Recalculate number of baskets and size of each basket for purchases\n",
    "purchases_df = add_aggregation_columns(purchases_df)\n",
    "print(f\">> Purchases: {purchases_df.shape}\")\n",
    "\n",
    "# Apply id2index, to work with indexes only\n",
    "inventory_df = map_ids_to_indexes(inventory_df, id2index)\n",
    "purchases_df = map_ids_to_indexes(purchases_df, id2index)\n",
    "evaluation_df = map_ids_to_indexes(evaluation_df, id2index)\n",
    "print(\">> Mapping applied\")\n",
    "\n",
    "# Add cluster id information\n",
    "inventory_df[\"cluster_id\"] = inventory_df[\"artwork_idx\"].apply(\n",
    "    lambda idx: best_clusterer.labels_[idx],\n",
    ")\n",
    "print(f\">> Inventory: {inventory_df.shape}\")\n",
    "\n",
    "# Create helper mapping from idx to data\n",
    "print(\"\\nCreating mappings from index to data\")\n",
    "artist_by_idx = np.full((features.shape[0],), -1)\n",
    "for artwork_idx, artist_id in inventory_df.set_index(\"artwork_idx\").to_dict()[\"artist_id\"].items():\n",
    "    artist_by_idx[artwork_idx] = artist_id\n",
    "cluster_by_idx = best_clusterer.labels_\n",
    "\n",
    "# Create helper mapping from data to idxs\n",
    "print(\"\\nCreating mappings from data to index\")\n",
    "artistId2artworkIndexes = inventory_df.groupby(\"artist_id\")[\"artwork_idx\"].apply(list).to_dict()\n",
    "clustId2artIndexes = dict()\n",
    "for i, cluster in enumerate(cluster_by_idx):\n",
    "    if cluster not in clustId2artIndexes:\n",
    "        clustId2artIndexes[cluster] = list()\n",
    "    clustId2artIndexes[cluster].append(i)\n",
    "\n",
    "print(\"\\nCreating helpers instances...\")\n",
    "# Creating hashes container for duplicates detection\n",
    "hashes_container = HashesContainer()\n",
    "# Creating custom score helpers\n",
    "vissimhandler = VisualSimilarityHandler(best_clusterer.labels_, features)\n",
    "hybrid_scorer = HybridScorer(vissimhandler, artist_by_idx, artist_boost=ARTIST_BOOST)\n",
    "\n",
    "# Sampling constants\n",
    "print(\"\\nCalculating important values...\")\n",
    "N_REAL_STRATEGIES = 2\n",
    "N_FAKE_STRATEGIES = 2\n",
    "print(f\">> There are {N_REAL_STRATEGIES} real strategies and {N_FAKE_STRATEGIES} fake strategies\")\n",
    "N_SAMPLES_PER_REAL_STRAT_TRAIN = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_TRAIN / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_REAL_STRAT_VALID = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_VALID / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_TRAIN = ceil(FAKE_COEF * TOTAL_SAMPLES_TRAIN / N_FAKE_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_VALID = ceil(FAKE_COEF * TOTAL_SAMPLES_VALID / N_FAKE_STRATEGIES)\n",
    "N_USERS = purchases_df[\"customer_id\"].nunique()\n",
    "N_ITEMS = len(embedding)\n",
    "print(f\">> N_USERS = {N_USERS} | N_ITEMS = {N_ITEMS}\")\n",
    "\n",
    "# Actual sampling section\n",
    "print(\"\\nCreating samples using custom strategies\")\n",
    "strategy_handler = StrategyHandler(\n",
    "    vissimhandler, hybrid_scorer,\n",
    "    clustId2artIndexes, cluster_by_idx,\n",
    "    artistId2artworkIndexes, artist_by_idx,\n",
    "    threshold=FINE_GRAINED_THRESHOLD,\n",
    "    confidence_margin=CONFIDENCE_MARGIN,\n",
    ")\n",
    "\n",
    "print(\">> Strategy #1: Given real profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_1 = strategy_handler.strategy_1(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_1) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_1 = strategy_handler.strategy_1(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_1) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #1 Training samples ({len(samples_train_1)}) and validation samples ({len(samples_valid_1)})\")\n",
    "\n",
    "print(\">> Strategy #2: Given fake profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_2 = strategy_handler.strategy_2(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_2) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_2 = strategy_handler.strategy_2(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_2) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #2: Training samples ({len(samples_train_2)}) and validation samples ({len(samples_valid_2)})\")\n",
    "\n",
    "print(\">> Strategy #3: Given real profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_3 = strategy_handler.strategy_3(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_3) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_3 = strategy_handler.strategy_3(\n",
    "    purchases_df.copy(),  # purchases_df\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_3) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #3: Training samples ({len(samples_train_3)}) and validation samples ({len(samples_valid_3)})\")\n",
    "\n",
    "print(\">> Strategy #4: Given fake profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_4 = strategy_handler.strategy_4(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_4) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_4 = strategy_handler.strategy_4(\n",
    "    embedding,  # embedding\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_4) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #4: Training samples ({len(samples_train_4)}) and validation samples ({len(samples_valid_4)})\")\n",
    "\n",
    "# Log out detected collisions\n",
    "print(f\">> Total hash collisions: {hashes_container.collisions}\")\n",
    "print(f\">> Total visual collisions: {vissimhandler.count}\")\n",
    "\n",
    "# Merge triples into a single list\n",
    "print(\"\\nMerging strategies samples into a single list\")\n",
    "TRAINING_DATA = [samples_train_1, samples_train_2, samples_train_3, samples_train_4]\n",
    "for i, samples in enumerate(TRAINING_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "TRAINING_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in TRAINING_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Training samples: {len(TRAINING_DATA)}\")\n",
    "# Merge strategies samples\n",
    "VALIDATION_DATA = [samples_valid_1, samples_valid_2, samples_valid_3, samples_valid_4]\n",
    "for i, samples in enumerate(VALIDATION_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):07d} | Sample: {samples[0] if samples else None}\")\n",
    "VALIDATION_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in VALIDATION_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Validation samples: {len(VALIDATION_DATA)}\")\n",
    "\n",
    "# Search for duplicated hashes\n",
    "print(f\"\\nNaive triples validation and looking for duplicates...\")\n",
    "validation_hash_check = HashesContainer()\n",
    "all_samples = [\n",
    "    triple\n",
    "    for subset in (TRAINING_DATA, VALIDATION_DATA)\n",
    "    for triple in subset\n",
    "]\n",
    "customer_ids = purchases_df[\"customer_id\"].unique()\n",
    "user_data = dict()\n",
    "for triple in tqdm(all_samples, desc=\"Naive validation\"):\n",
    "    profile, pi, ni, ui = triple\n",
    "    assert validation_hash_check.enroll(pre_hash((profile, pi, ni)))\n",
    "    assert 0 <= pi < N_ITEMS\n",
    "    assert 0 <= ni < N_ITEMS\n",
    "    assert pi != ni\n",
    "    assert not vissimhandler.same(pi, ni)\n",
    "    if ui == -1:\n",
    "        continue\n",
    "    assert ui in customer_ids\n",
    "    if not ui in user_data:\n",
    "        user = purchases_df[purchases_df[\"customer_id\"] == ui]\n",
    "        user_data[ui] = set(np.concatenate(user[\"artwork_idx\"].values))\n",
    "    user_artworks = user_data[ui]\n",
    "    assert all(i in user_artworks for i in profile)\n",
    "    spi = hybrid_scorer.get_score(ui, user_artworks, pi)\n",
    "    sni = hybrid_scorer.get_score(ui, user_artworks, ni)\n",
    "    assert spi > sni\n",
    "print(\">> No duped hashes found\")\n",
    "\n",
    "print(\"\\nCreating output DataFrames (train and valid)...\")\n",
    "# Training dataframe\n",
    "df_train = pd.DataFrame(TRAINING_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_train = df_train.drop(\"ui\", axis=1)\n",
    "print(f\">> Saving training samples ({OUTPUT_TRAIN_PATH})\")\n",
    "df_train.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "# Validation dataframe\n",
    "df_validation = pd.DataFrame(VALIDATION_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_validation = df_validation.drop(\"ui\", axis=1)\n",
    "print(f\">> Saving validation samples in ({OUTPUT_VALID_PATH})\")\n",
    "df_validation.to_csv(OUTPUT_VALID_PATH, index=False)\n",
    "\n",
    "print(\"\\nCreating output json file (evaluation)...\")\n",
    "evaluation_baskets = dict()\n",
    "for uid in evaluation_df[\"customer_id\"]:\n",
    "    # Retrieve data\n",
    "    purchases = purchases_df[purchases_df[\"customer_id\"] == uid]\n",
    "    evaluation = evaluation_df[evaluation_df[\"customer_id\"] == uid]\n",
    "    # Format\n",
    "    profile = list(set(purchases[\"artwork_idx\"].map(list).sum()))\n",
    "    basket = list(set(evaluation[\"artwork_idx\"].map(list).sum()))\n",
    "    timestamp = int(evaluation_df[evaluation_df[\"customer_id\"] == uid][\"timestamp\"].values[0])\n",
    "    # Store\n",
    "    evaluation_baskets[uid] = {\n",
    "        \"profile\": profile,\n",
    "        \"evaluation_basket\": basket,\n",
    "        \"evaluation_timestamp\": timestamp,\n",
    "    }\n",
    "print(f\">> Saving evaluation samples in ({OUTPUT_EVAL_PATH})\")\n",
    "with open(OUTPUT_EVAL_PATH, \"w\") as file:\n",
    "    json.dump(evaluation_baskets, file, indent=4)\n",
    "\n",
    "# Finished\n",
    "print(\"\\nDone\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
