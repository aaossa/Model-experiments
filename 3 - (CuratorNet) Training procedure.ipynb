{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul  4 15:32:03 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.36       Driver Version: 440.36       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 64%   72C    P2   249W / 280W |  10933MiB / 11170MiB |    100%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "|  0%   40C    P5    18W / 280W |      0MiB / 11178MiB |      3%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      7519      C   python                                     10923MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colaboratory setup\n",
    "\n",
    "Clone repository contents in VM and install dependencies using the script:\n",
    "\n",
    "```python\n",
    "# (1) Replace contents of VM\n",
    "!rm -rf sample_data\n",
    "# (Replace username and password/token)\n",
    "!git clone --single-branch --branch master https://username:password@github.com/aaossa/CuratorNet-experiments.git\n",
    "!cp -a CuratorNet-experiments/. .\n",
    "!rm -r CuratorNet-experiments/\n",
    "# Setup VM using script\n",
    "!chmod +x ./scripts/colaboratory.sh\n",
    "!./scripts/colaboratory.sh requirements/dev.txt\n",
    "```\n",
    "\n",
    "Mount Google Drive in case the data is available there:\n",
    "\n",
    "```python\n",
    "# (2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "Extract data in the right folder:\n",
    "\n",
    "```python\n",
    "# (3) Bring actual data to VM\n",
    "# Extract data from mounted drive to data folder\n",
    "!tar -xvzf \"/content/drive/My Drive/path_to_data/data.tar.gz\" -C data/UGallery\n",
    "```\n",
    "\n",
    "**Important:** Restart the VM after following the steps to make sure you're using the right version of the declared requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:53.542195Z",
     "start_time": "2020-04-06T15:20:53.367196Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "\n",
    "from datasets import UGalleryDataset\n",
    "from models import CuratorNet\n",
    "from samplers import SameProfileSizeBatchSampler\n",
    "from trainers import train_with_batch_samplers, train_with_dataloaders\n",
    "from utils.memory import max_memory_stats, memory_report\n",
    "\n",
    "\n",
    "# Parameters\n",
    "RNG_SEED = 0\n",
    "EMBEDDING_PATH = os.path.join(\"data\", \"UGallery\", \"ugallery_embedding.npy\")\n",
    "TRAINING_PATH = os.path.join(\"data\", \"UGallery\", \"train.csv\")\n",
    "VALIDATION_PATH = os.path.join(\"data\", \"UGallery\", \"validation.csv\")\n",
    "SUMMARY_WRITER_DIR = os.path.join(\"runs\", \"CuratorNet\")\n",
    "CHECKPOINTS_DIR = os.path.join(\"checkpoints\", \"CuratorNet\")\n",
    "USE_GPU = True\n",
    "\n",
    "# Parameters (training)\n",
    "SETTINGS = {\n",
    "    \"batch_sampler:batch_size\": 4096 * 3,\n",
    "    \"batch_sampler:profile_items_per_batch\": 60_000,\n",
    "    \"dataloader:num_workers\": os.cpu_count(),\n",
    "    \"dataloader:pin_memory\": True,\n",
    "    \"optimizer:lr\": 0.0001,\n",
    "    \"optimizer:weight_decay\": 0.0001,\n",
    "    \"scheduler:factor\": 0.6,\n",
    "    \"scheduler:patience\": 2,\n",
    "    \"scheduler:threshold\": 1e-4,  # Default value (https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "    \"train:max_epochs\": 150,\n",
    "    \"train:max_lrs\": 10,\n",
    "    \"train:non_blocking\": True,\n",
    "    \"train:train_per_valid_times\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using random seed...\n",
      "\n",
      "Loading embedding from file... (data/UGallery/ugallery_embedding.npy)\n",
      "\n",
      "Reshape embedding\n",
      "\n",
      "Initialize DataLoaders\n",
      ">> Training dataset: 10000494\n",
      ">> Training dataloader: 893\n",
      ">> Validation dataset: 502068\n",
      ">> Validation dataloader: 71\n",
      "\n",
      "Initialize model\n",
      "\n",
      "Setting up training\n",
      "\n",
      "Memory report\n",
      "Main process PID: 12618\n",
      "CPU RAM free: 53.3 GB | Proc. size: 2.5 GB\n",
      "GPU 0 (GeForce GTX 1080 Ti)\n",
      "GPU RAM free: 237 MB | Used: 10933 MB | Util.: 98% | Total: 11170 MB\n",
      "GPU 1 (GeForce GTX 1080 Ti)\n",
      "GPU RAM free: 10379 MB | Used: 799 MB | Util.: 7% | Total: 11178 MB\n",
      "\n",
      "Max memory stats\n",
      "Device: 'cuda:0'\n",
      "Max memory allocated: 222.6 MB\n",
      "Max memory reserved:  241.2 MB\n",
      "\n",
      "Training\n",
      "CPU times: user 7.99 s, sys: 912 ms, total: 8.9 s\n",
      "Wall time: 9.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed...\")\n",
    "    random.seed(RNG_SEED)\n",
    "    torch.manual_seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n",
    "\n",
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
    "embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "\n",
    "# Reshape embedding\n",
    "print(\"\\nReshape embedding\")\n",
    "new_shape = (embedding.shape[0], embedding[0, 1].shape[0])\n",
    "embedding = np.concatenate(embedding[:, 1]).reshape(*new_shape)\n",
    "\n",
    "# DataLoaders initialization\n",
    "print(\"\\nInitialize DataLoaders\")\n",
    "# Training DataLoader\n",
    "train_dataset = UGalleryDataset(\n",
    "    csv_file=TRAINING_PATH,\n",
    ")\n",
    "print(f\">> Training dataset: {len(train_dataset)}\")\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_batch_sampler = SameProfileSizeBatchSampler(\n",
    "    sampler=train_sampler,\n",
    "    batch_size=SETTINGS[\"batch_sampler:batch_size\"],\n",
    "    profile_items_per_batch=SETTINGS[\"batch_sampler:profile_items_per_batch\"],\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=train_batch_sampler,\n",
    "    num_workers=SETTINGS[\"dataloader:num_workers\"],\n",
    "    pin_memory=SETTINGS[\"dataloader:pin_memory\"],\n",
    ")\n",
    "print(f\">> Training dataloader: {len(train_dataloader)}\")\n",
    "# Validation DataLoader\n",
    "valid_dataset = UGalleryDataset(\n",
    "    csv_file=VALIDATION_PATH,\n",
    ")\n",
    "print(f\">> Validation dataset: {len(valid_dataset)}\")\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_batch_sampler = SameProfileSizeBatchSampler(\n",
    "    sampler=valid_sampler,\n",
    "    batch_size=SETTINGS[\"batch_sampler:batch_size\"],\n",
    "    profile_items_per_batch=SETTINGS[\"batch_sampler:profile_items_per_batch\"],\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    sampler=valid_batch_sampler,\n",
    "    num_workers=SETTINGS[\"dataloader:num_workers\"],\n",
    "    pin_memory=SETTINGS[\"dataloader:pin_memory\"],\n",
    ")\n",
    "print(f\">> Validation dataloader: {len(valid_dataloader)}\")\n",
    "# Model initialization\n",
    "print(\"\\nInitialize model\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "if torch.cuda.is_available() != USE_GPU:\n",
    "    print((f\"\\nNotice: Not using GPU - \"\n",
    "           f\"Cuda available ({torch.cuda.is_available()}) \"\n",
    "           f\"does not match USE_GPU ({USE_GPU})\"\n",
    "    ))\n",
    "model = CuratorNet(torch.Tensor(embedding), input_size=embedding.shape[1]).to(device)\n",
    "\n",
    "# Training setup\n",
    "print(\"\\nSetting up training\")\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=SETTINGS[\"optimizer:lr\"],\n",
    "    weight_decay=SETTINGS[\"optimizer:weight_decay\"],\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=SETTINGS[\"scheduler:factor\"],\n",
    "    patience=SETTINGS[\"scheduler:patience\"], verbose=True,\n",
    "    threshold=SETTINGS[\"scheduler:threshold\"],\n",
    ")\n",
    "\n",
    "# Crurent memory status\n",
    "print(\"\\nMemory report\")\n",
    "memory_report()\n",
    "print(\"\\nMax memory stats\")\n",
    "max_memory_stats(device)\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Checkpoints stored at... checkpoints/CuratorNet/CuratorNet_2020-07-04-15-32-13.tar\n",
      ">> Summary writer data stored at... runs/CuratorNet/CuratorNet_2020-07-04-15-32-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0976c220483242fe9a7fcfa71b4cbd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=150.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918ea26b92644880a7eb7742164bfa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=893.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cde67a886d411c962e16fee0cb3691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=71.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    13: reducing learning rate of group 0 to 6.0000e-05.\n",
      "Epoch    22: reducing learning rate of group 0 to 3.6000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 2.1600e-05.\n",
      "Epoch    47: reducing learning rate of group 0 to 1.2960e-05.\n",
      "Epoch    52: reducing learning rate of group 0 to 7.7760e-06.\n",
      "Epoch    60: reducing learning rate of group 0 to 4.6656e-06.\n",
      "Epoch    66: reducing learning rate of group 0 to 2.7994e-06.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.6796e-06.\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0078e-06.\n",
      "Epoch    82: reducing learning rate of group 0 to 6.0466e-07.\n",
      ">> Reached max different lrs ([0.0001, 6e-05, 3.6e-05, 2.16e-05, 1.296e-05, 7.776e-06, 4.6656e-06, 2.79936e-06, 1.679616e-06, 1.0077696e-06])\n",
      "\n",
      "\n",
      "\n",
      ">> Training completed in 46m 37s\n",
      ">> Best validation accuracy: ~98.900%\n",
      ">> Copy last model\n",
      ">> Load best model\n",
      ">> Save last state\n",
      "CPU times: user 39min 2s, sys: 7min 31s, total: 46min 33s\n",
      "Wall time: 46min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training\n",
    "best_model, best_acc, best_epoch, last_model = train_with_batch_samplers(\n",
    "    model, device, criterion, optimizer, scheduler,\n",
    "    {\"train\": train_batch_sampler, \"validation\": valid_batch_sampler},\n",
    "    max_epochs=SETTINGS[\"train:max_epochs\"],\n",
    "    max_lrs=SETTINGS[\"train:max_lrs\"],\n",
    "    train_per_valid_times=SETTINGS[\"train:train_per_valid_times\"],\n",
    "    checkpoint_dir=CHECKPOINTS_DIR, writer_dir=SUMMARY_WRITER_DIR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory report\n",
      "Main process PID: 12618\n",
      "CPU RAM free: 53.0 GB | Proc. size: 2.5 GB\n",
      "GPU 0 (GeForce GTX 1080 Ti)\n",
      "GPU RAM free: 237 MB | Used: 10933 MB | Util.: 98% | Total: 11170 MB\n",
      "GPU 1 (GeForce GTX 1080 Ti)\n",
      "GPU RAM free: 6591 MB | Used: 4587 MB | Util.: 41% | Total: 11178 MB\n",
      "\n",
      "Max memory stats\n",
      "Device: 'cuda:0'\n",
      "Max memory allocated: 1.9 GB\n",
      "Max memory reserved:  4.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Crurent memory status\n",
    "print(\"\\nMemory report\")\n",
    "memory_report()\n",
    "print(\"\\nMax memory stats\")\n",
    "max_memory_stats(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best ACC 0.9889955145518137 reached at epoch 78\n",
      "CuratorNet(\n",
      "  (embedding): Embedding(13297, 4096)\n",
      "  (selu_common1): Linear(in_features=4096, out_features=200, bias=True)\n",
      "  (selu_common2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (maxpool): AdaptiveMaxPool2d(output_size=(1, 200))\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 200))\n",
      "  (selu_pu1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (selu_pu2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (selu_pu3): Linear(in_features=300, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Final result\n",
    "print(f\"\\nBest ACC {best_acc} reached at epoch {best_epoch}\")\n",
    "print(best_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.3",
   "language": "python",
   "name": "3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
