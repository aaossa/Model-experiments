{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import VisRank\n",
    "from utils.data import extract_embedding\n",
    "from utils.metrics import (\n",
    "    auc_exact,\n",
    "    nDCG,\n",
    "    precision,\n",
    "    recall,\n",
    "    reciprocal_rank,\n",
    ")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "DATASET = \"Wikimedia\"\n",
    "assert DATASET in [\"UGallery\", \"Wikimedia\", \"Pinterest\", \"Tradesy\"]\n",
    "FEATURE_EXTRACTOR = \"resnet50\"\n",
    "assert FEATURE_EXTRACTOR in [\"resnet50\"]\n",
    "FEATURE_EXTRACTOR_VERSION = \"imagenet\"\n",
    "assert FEATURE_EXTRACTOR_VERSION in [\"imagenet\", \"places365\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode\n",
    "MODE_PROFILE = \"profile\"\n",
    "\n",
    "# Paths (general)\n",
    "EMBEDDING_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}_embedding-{FEATURE_EXTRACTOR}.npy\")\n",
    "EVALUATION_PATH = os.path.join(\"data\", DATASET, f\"{MODE_PROFILE}-evaluation.csv\")\n",
    "\n",
    "# Paths (images)\n",
    "IMAGES_DIR = None\n",
    "if DATASET == \"UGallery\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"workspace\", \"Ugallery\", \"images\")\n",
    "elif DATASET == \"Wikimedia\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"wikimedia\", \"images\", \"img\")\n",
    "elif DATASET == \"Pinterest\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"pinterest_iccv\", \"images\")\n",
    "elif DATASET == \"Tradesy\":\n",
    "    print(\"Tradesy dataset not supported at the moment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataframe\n",
    "print(\"\\nLoad evaluation dataframe\")\n",
    "evaluation_df = pd.read_csv(EVALUATION_PATH)\n",
    "# Transform lists from str to int\n",
    "string_to_list = lambda s: list(map(int, s.split()))\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].apply(\n",
    "    lambda s: string_to_list(s) if isinstance(s, str) else s,\n",
    ")\n",
    "evaluation_df[\"predict\"] = evaluation_df[\"predict\"].apply(\n",
    "    lambda s: string_to_list(s) if isinstance(s, str) else s,\n",
    ")\n",
    "# Group evaluations by profile and user\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].map(tuple)\n",
    "evaluation_df = evaluation_df.groupby([\"profile\", \"user_id\"]).agg({\"predict\": sum}).reset_index()\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].map(list)\n",
    "print(f\">> Evaluation: {evaluation_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].map(tuple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass testing for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all\n",
    "# If True, ranks every item including already consumed items\n",
    "# If False, ranks ALL - PROFILE (consumed) + PREDICT (ground truth)\n",
    "PREDICT_ALL = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "COMBINATIONS = [\n",
    "    (p1, p2, p3, p4)\n",
    "    for p1 in range(4)  # UNIT_CRITERIA\n",
    "    for p2 in range(2)  # UNIQUE_DETECTORS_ONLY\n",
    "    for p3 in range(5)  # LAYER_CRITERIA\n",
    "    for p4 in range(2)  # SAME_CONCEPT_UNITS_CRITERIA\n",
    "]\n",
    "# random.shuffle(COMBINATIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (p1, p2, p3, p4) in tqdm(COMBINATIONS):\n",
    "    print(p1, p2, p3, p4)\n",
    "    \n",
    "    # Check if embedding exists\n",
    "    EMBEDDING_PATH = os.path.join(\n",
    "        \"..\", \"..\",\n",
    "        \"embeddings\",\n",
    "        \"{}_{}_{}\".format(FEATURE_EXTRACTOR, FEATURE_EXTRACTOR_VERSION, DATASET.lower()),\n",
    "        \"{}{}{}{}.npy\".format(p1, p2, p3, p4),\n",
    "    )\n",
    "    if not os.path.exists(EMBEDDING_PATH):\n",
    "        print(\"Embedding not found:\", p1, p2, p3, p4)\n",
    "        continue\n",
    "\n",
    "        \n",
    "    # Load embedding from file\n",
    "    print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
    "    embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "\n",
    "    # Extract features and \"id2index\" mapping\n",
    "    features, _, item_index2fn = extract_embedding(embedding, verbose=True)\n",
    "    del embedding  # Release some memory\n",
    "\n",
    "    # Model initialization\n",
    "    model = VisRank(\n",
    "        features,  # Embedding\n",
    "        similarity_method=cosine_similarity,  # Similarity measure\n",
    "    )\n",
    "    print(\"\\nModel ready...\")\n",
    "\n",
    "    # Metrics\n",
    "    N_EVALS = len(evaluation_df.index)\n",
    "    # Area Under the Curve (AUC)\n",
    "    AUC = np.zeros(N_EVALS, dtype=float)\n",
    "    # Reciprocal Rank (RR)\n",
    "    RR = np.zeros(N_EVALS, dtype=float)\n",
    "    # Recall\n",
    "    R20 = np.zeros(N_EVALS, dtype=float)\n",
    "    R100 = np.zeros(N_EVALS, dtype=float)\n",
    "    R200 = np.zeros(N_EVALS, dtype=float)\n",
    "    # Precision\n",
    "    P20 = np.zeros(N_EVALS, dtype=float)\n",
    "    P100 = np.zeros(N_EVALS, dtype=float)\n",
    "    P200 = np.zeros(N_EVALS, dtype=float)\n",
    "    # Normalized discounted cumulative gain (nDCG)\n",
    "    N20 = np.zeros(N_EVALS, dtype=float)\n",
    "    N100 = np.zeros(N_EVALS, dtype=float)\n",
    "    N200 = np.zeros(N_EVALS, dtype=float)\n",
    "    PROFILE_SIZES = np.zeros(N_EVALS, dtype=int)\n",
    "    N_ITEMS = len(features)\n",
    "\n",
    "    # Evaluation loop\n",
    "    for i, row in tqdm(enumerate(evaluation_df.itertuples()), total=len(evaluation_df.index)):\n",
    "        # Load data into tensors\n",
    "        profile = np.array(row.profile)\n",
    "        user_id = int(row.user_id)\n",
    "        predict = row.predict\n",
    "        # Prediction\n",
    "        indexes, _ = model.most_similar_to_profile(profile, k=None, method=\"maximum\", include_consumed=True)\n",
    "        if not PREDICT_ALL:\n",
    "            indexes = np.delete(\n",
    "                indexes,\n",
    "                np.where(np.isin(indexes, profile) & ~np.isin(indexes, predict)),\n",
    "            )\n",
    "        # Ranking\n",
    "        pos_of_evals = torch.Tensor(np.where(np.isin(indexes, predict))).flatten()\n",
    "        # Store metrics\n",
    "        AUC[i] = auc_exact(pos_of_evals, N_ITEMS)\n",
    "        RR[i] = reciprocal_rank(pos_of_evals)\n",
    "        R20[i] = recall(pos_of_evals, 20)\n",
    "        P20[i] = precision(pos_of_evals, 20)\n",
    "        N20[i] = nDCG(pos_of_evals, 20)\n",
    "        R100[i] = recall(pos_of_evals, 100)\n",
    "        P100[i] = precision(pos_of_evals, 100)\n",
    "        N100[i] = nDCG(pos_of_evals, 100)\n",
    "        R200[i] = recall(pos_of_evals, 200)\n",
    "        P200[i] = precision(pos_of_evals, 200)\n",
    "        N200[i] = nDCG(pos_of_evals, 200)\n",
    "        PROFILE_SIZES[i] = len(row.profile)\n",
    "        \n",
    "    ROWS.append(\n",
    "        (\n",
    "            p1, p2, p3, p4,\n",
    "            AUC.mean(), RR.mean(),\n",
    "            R20.mean(), P20.mean(), N20.mean(),\n",
    "            R100.mean(), P100.mean(), N100.mean(),\n",
    "            R200.mean(), P200.mean(), N200.mean(),\n",
    "        )\n",
    "    )\n",
    "    print(ROWS[-1])\n",
    "\n",
    "# Wikimedia: ~1.25 hrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = pd.DataFrame(\n",
    "    ROWS,\n",
    "    columns=[\n",
    "        \"p1\", \"p2\", \"p3\", \"p4\",\n",
    "        \"AUC\", \"RR\",\n",
    "        \"R20\", \"P20\", \"N20\",\n",
    "        \"R100\", \"P100\", \"N100\",\n",
    "        \"R200\", \"P200\", \"N200\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = os.path.join(\"..\", \"..\", \"embeddings\", \"results\", f\"{FEATURE_EXTRACTOR}_{FEATURE_EXTRACTOR_VERSION}_{DATASET.lower()}.csv\")\n",
    "RESULTS.to_csv(RESULTS_PATH)\n",
    "print(RESULTS_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.5",
   "language": "python",
   "name": "3.8.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
