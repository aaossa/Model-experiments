{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colaboratory setup\n",
    "\n",
    "Clone repository contents in VM and install dependencies using the script:\n",
    "\n",
    "```python\n",
    "# (1) Replace contents of VM\n",
    "!rm -rf sample_data\n",
    "# (Replace username and password/token)\n",
    "!git clone --single-branch --branch master https://username:password@github.com/aaossa/CuratorNet-experiments.git\n",
    "!cp -a CuratorNet-experiments/. .\n",
    "!rm -r CuratorNet-experiments/\n",
    "# Setup VM using script\n",
    "!chmod +x ./scripts/colaboratory.sh\n",
    "!./scripts/colaboratory.sh requirements/dev.txt\n",
    "```\n",
    "\n",
    "Mount Google Drive in case the data is available there:\n",
    "\n",
    "```python\n",
    "# (2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "Extract data in the right folder:\n",
    "\n",
    "```python\n",
    "# (3) Bring actual data to VM\n",
    "# Extract data from mounted drive to data folder\n",
    "!tar -xvzf \"/content/drive/My Drive/path_to_data/data.tar.gz\" -C data/UGallery\n",
    "```\n",
    "\n",
    "**Important:** Restart the VM after following the steps to make sure you're using the right version of the declared requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import CuratorNet\n",
    "from utils.metrics import auc_exact, nDCG, precision, recall\n",
    "from utils.ugallery.data import get_evaluation_dataframe\n",
    "\n",
    "\n",
    "# Parameters\n",
    "CHECKPOINT_PATH = os.path.join(\"checkpoints\", \"CuratorNet\", \"CuratorNet_2020-07-04-15-32-13.tar\")\n",
    "EMBEDDING_PATH = os.path.join(\"data\", \"UGallery\", \"ugallery_embedding.npy\")\n",
    "EVALUATION_PATH = os.path.join(\"data\", \"UGallery\", \"evaluation.csv\")\n",
    "FULL_INVENTORY_MODE = False\n",
    "USE_GPU = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:53.542195Z",
     "start_time": "2020-04-06T15:20:53.367196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
    "embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "\n",
    "# Reshape embedding\n",
    "print(\"\\nReshape embedding\")\n",
    "new_shape = (embedding.shape[0], embedding[0, 1].shape[0])\n",
    "embedding = np.concatenate(embedding[:, 1]).reshape(*new_shape)\n",
    "\n",
    "# Model initialization\n",
    "print(\"\\nInitialize model\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "if torch.cuda.is_available() != USE_GPU:\n",
    "    print((f\"\\nNotice: Not using GPU - \"\n",
    "           f\"Cuda available ({torch.cuda.is_available()}) \"\n",
    "           f\"does not match USE_GPU ({USE_GPU})\"\n",
    "    ))\n",
    "model = CuratorNet(torch.Tensor(embedding), input_size=embedding.shape[1]).to(device)\n",
    "\n",
    "# Loading checkpoint\n",
    "print(\"\\nLoading checkpoint\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "print(f\">> Best epoch: {checkpoint['epoch']} | Best accuracy: {checkpoint['accuracy']}\")\n",
    "\n",
    "# Change model mode to eval\n",
    "print(\"\\nChanging model mode to eval\")\n",
    "model.eval()\n",
    "\n",
    "# Load evaluation dataframe\n",
    "print(\"\\nLoad evaluation dataframe\")\n",
    "evaluation_df = get_evaluation_dataframe(EVALUATION_PATH)\n",
    "print(f\">> Evaluation: {evaluation_df.shape}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Metrics\n",
    "INVENTORY_IDXS = list()\n",
    "AUC = list()\n",
    "R20, P20, N20 = list(), list(), list()\n",
    "R100, P100, N100 = list(), list(), list()\n",
    "PROFILE_SIZES = list()\n",
    "\n",
    "for row in tqdm(evaluation_df.itertuples(), total=len(evaluation_df.index)):\n",
    "    if row.event == \"inventory\":\n",
    "        # Add item to inventory if necessary\n",
    "        INVENTORY_IDXS.append(int(row.artwork_id))\n",
    "    elif row.event == \"purchase\":\n",
    "        # Remove item from inventory\n",
    "        for item in row.shopping_cart:\n",
    "            if item in INVENTORY_IDXS:\n",
    "                INVENTORY_IDXS.remove(item)\n",
    "    elif row.event == \"evaluation\":\n",
    "        # Calculate metrics for evaluation\n",
    "        predict_idxs = row.predict\n",
    "        if FULL_INVENTORY_MODE:\n",
    "            inventory_idxs = list(range(len(embedding)))\n",
    "            inventory = None\n",
    "        else:\n",
    "            inventory_idxs = list(set(INVENTORY_IDXS + predict_idxs))\n",
    "            inventory = torch.tensor(inventory_idxs, device=device).unsqueeze(0)\n",
    "        # Prediction\n",
    "        profile_idxs = row.profile\n",
    "        profile = torch.tensor(profile_idxs, device=device).unsqueeze(0)\n",
    "        scores = model.recommend(profile, inventory).cpu().numpy()\n",
    "        # Ranking\n",
    "        idx_of_evals = np.nonzero(np.in1d(inventory_idxs, predict_idxs))[0]\n",
    "        pos_of_evals = np.nonzero(np.in1d(np.argsort(scores)[::-1], idx_of_evals))[0]\n",
    "        # Addition to metrics\n",
    "        AUC.append(auc_exact(pos_of_evals, len(inventory_idxs)))\n",
    "        R20.append(recall(pos_of_evals, 20))\n",
    "        P20.append(precision(pos_of_evals, 20))\n",
    "        N20.append(nDCG(pos_of_evals, 20))\n",
    "        R100.append(recall(pos_of_evals, 100))\n",
    "        P100.append(precision(pos_of_evals, 100))\n",
    "        N100.append(nDCG(pos_of_evals, 100))\n",
    "        PROFILE_SIZES.append(len(profile_idxs))\n",
    "        # After prediction\n",
    "        for item in predict_idxs:\n",
    "            if item in INVENTORY_IDXS:\n",
    "                INVENTORY_IDXS.remove(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display stats\n",
    "print(f\"AVG AUC = {sum(AUC) / len(AUC)}\")\n",
    "print(f\"AVG R20 = {sum(R20) / len(R20)}\")\n",
    "print(f\"AVG P20 = {sum(P20) / len(P20)}\")\n",
    "print(f\"AVG NDCG20 = {sum(N20) / len(N20)}\")\n",
    "print(f\"AVG R100 = {sum(R100) / len(R100)}\")\n",
    "print(f\"AVG P100 = {sum(P100) / len(P100)}\")\n",
    "print(f\"AVG NDCG100 = {sum(N100) / len(N100)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.3",
   "language": "python",
   "name": "3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
