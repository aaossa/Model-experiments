{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import CuratorNet, VBPR\n",
    "from utils.data import extract_embedding\n",
    "from utils.metrics import auc_exact, nDCG, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colaboratory setup\n",
    "\n",
    "Clone repository contents in VM and install dependencies using the script:\n",
    "\n",
    "```python\n",
    "# (1) Replace contents of VM\n",
    "!rm -rf sample_data\n",
    "# (Replace username and password/token)\n",
    "!git clone --single-branch --branch master https://username:password@github.com/aaossa/CuratorNet-experiments.git\n",
    "!cp -a CuratorNet-experiments/. .\n",
    "!rm -r CuratorNet-experiments/\n",
    "# Setup VM using script\n",
    "!chmod +x ./scripts/colaboratory.sh\n",
    "!./scripts/colaboratory.sh requirements/dev.txt\n",
    "```\n",
    "\n",
    "Mount Google Drive in case the data is available there:\n",
    "\n",
    "```python\n",
    "# (2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "Extract data in the right folder:\n",
    "\n",
    "```python\n",
    "# (3) Bring actual data to VM\n",
    "# Extract data from mounted drive to data folder\n",
    "!tar -xvzf \"/content/drive/My Drive/dataset/dataset.tar.gz\" -C data/dataset\n",
    "```\n",
    "\n",
    "**Important:** Restart the VM after following the steps to make sure you're using the right version of the declared requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# * UGallery\n",
    "# * Wikimedia\n",
    "# * Pinterest\n",
    "DATASET = \"UGallery\"\n",
    "assert DATASET in [\"UGallery\", \"Wikimedia\", \"Pinterest\"]\n",
    "\n",
    "# Model\n",
    "# * CuratorNet\n",
    "# * VBPR\n",
    "MODEL = \"CuratorNet\"\n",
    "assert MODEL in [\"CuratorNet\", \"VBPR\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode\n",
    "# Use 'MODE_PROFILE = True' for CuratorNet-like training \n",
    "# Use 'MODE_PROFILE = False' for VBPR-like training\n",
    "MODE_PROFILE = MODEL in [\"CuratorNet\"]\n",
    "MODE_PROFILE = \"profile\" if MODE_PROFILE else \"user\"\n",
    "\n",
    "# Checkpoint (ex. 'CuratorNet_2020-08-07-23-59-50')\n",
    "CHECKPOINT = \"CuratorNet_2020-08-09-01-35-38\"\n",
    "if CHECKPOINT is not None:\n",
    "    assert CHECKPOINT.startswith(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (general)\n",
    "CHECKPOINT_PATH = os.path.join(\"checkpoints\", MODEL, f\"{CHECKPOINT}.tar\")\n",
    "EMBEDDING_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}_embedding.npy\")\n",
    "EVALUATION_PATH = os.path.join(\"data\", DATASET, f\"{MODE_PROFILE}-evaluation.csv\")\n",
    "\n",
    "# General constants\n",
    "RNG_SEED = 0\n",
    "USE_GPU = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed... ({RNG_SEED})\")\n",
    "    torch.manual_seed(RNG_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
    "embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "\n",
    "# Extract features and \"id2index\" mapping\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features, _ = extract_embedding(embedding, verbose=True)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "del embedding  # Release some memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataframe\n",
    "print(\"\\nLoad evaluation dataframe\")\n",
    "evaluation_df = pd.read_csv(EVALUATION_PATH)\n",
    "# Transform lists from str to int\n",
    "string_to_list = lambda s: list(map(int, s.split()))\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].apply(\n",
    "    lambda s: string_to_list(s) if isinstance(s, str) else s,\n",
    ")\n",
    "evaluation_df[\"predict\"] = evaluation_df[\"predict\"].apply(\n",
    "    lambda s: string_to_list(s) if isinstance(s, str) else s,\n",
    ")\n",
    "print(f\">> Evaluation: {evaluation_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create device instance\n",
    "print(\"\\nDevice initialization\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "if torch.cuda.is_available() != USE_GPU:\n",
    "    print((f\"\\nNotice: Not using GPU - \"\n",
    "           f\"Cuda available ({torch.cuda.is_available()}) \"\n",
    "           f\"does not match USE_GPU ({USE_GPU})\"\n",
    "    ))\n",
    "\n",
    "# Loading checkpoint\n",
    "if CHECKPOINT is not None:\n",
    "    print(\"\\nLoading checkpoint\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(\"cpu\"))\n",
    "    print(f\">> Best epoch: {checkpoint['epoch']} | Best accuracy: {checkpoint['accuracy']}\")\n",
    "\n",
    "# Model initialization\n",
    "print(\"\\nModel initialization\")\n",
    "model = None\n",
    "if MODEL == \"CuratorNet\":\n",
    "    model = CuratorNet(\n",
    "        torch.Tensor(features),  # Pretrained visual features\n",
    "        input_size=features.shape[1],  # Network input size\n",
    "    ).to(device)\n",
    "elif MODEL == \"VBPR\":\n",
    "    n_users = checkpoint[\"model\"][\"gamma_users.weight\"].size(0)\n",
    "    n_items = checkpoint[\"model\"][\"gamma_items.weight\"].size(0)\n",
    "    dim_gamma = checkpoint[\"model\"][\"gamma_users.weight\"].size(1)\n",
    "    dim_theta = checkpoint[\"model\"][\"theta_users.weight\"].size(1)\n",
    "    model = VBPR(\n",
    "        n_users, n_items,  # Number of users and items\n",
    "        torch.Tensor(features),  # Pretrained visual features\n",
    "        dim_gamma, dim_theta,  # Size of internal spaces\n",
    "    ).to(device)\n",
    "\n",
    "# Load state dict\n",
    "if CHECKPOINT is not None:\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    \n",
    "# Change model mode to eval\n",
    "print(\"\\nChanging model mode to eval\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "AUC = list()\n",
    "R20, P20, N20 = list(), list(), list()\n",
    "R100, P100, N100 = list(), list(), list()\n",
    "PROFILE_SIZES = list()\n",
    "INVENTORY_IDXS = list(range(len(features)))\n",
    "\n",
    "for row in tqdm(evaluation_df.itertuples(), total=len(evaluation_df.index)):\n",
    "    # Prediction\n",
    "    if MODE_PROFILE == \"profile\":\n",
    "        profile = torch.tensor(row.profile, device=device).unsqueeze(0)\n",
    "        scores = model.recommend(profile, None).cpu().numpy()\n",
    "    elif MODE_PROFILE == \"user\":\n",
    "        user_id = torch.tensor([int(row.user_id)], device=device)\n",
    "        scores = model.recommend(user_id, None).squeeze().cpu().numpy()\n",
    "    # Ranking\n",
    "    idx_of_evals = np.nonzero(np.in1d(INVENTORY_IDXS, row.predict))[0]\n",
    "    pos_of_evals = np.nonzero(np.in1d(np.argsort(scores)[::-1], idx_of_evals))[0]\n",
    "    # Store metrics\n",
    "    AUC.append(auc_exact(pos_of_evals, len(INVENTORY_IDXS)))\n",
    "    R20.append(recall(pos_of_evals, 20))\n",
    "    P20.append(precision(pos_of_evals, 20))\n",
    "    N20.append(nDCG(pos_of_evals, 20))\n",
    "    R100.append(recall(pos_of_evals, 100))\n",
    "    P100.append(precision(pos_of_evals, 100))\n",
    "    N100.append(nDCG(pos_of_evals, 100))\n",
    "    PROFILE_SIZES.append(len(row.profile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display stats\n",
    "print(f\"AVG AUC = {sum(AUC) / len(AUC)}\")\n",
    "print(f\"AVG R20 = {sum(R20) / len(R20)}\")\n",
    "print(f\"AVG P20 = {sum(P20) / len(P20)}\")\n",
    "print(f\"AVG NDCG20 = {sum(N20) / len(N20)}\")\n",
    "print(f\"AVG R100 = {sum(R100) / len(R100)}\")\n",
    "print(f\"AVG P100 = {sum(P100) / len(P100)}\")\n",
    "print(f\"AVG NDCG100 = {sum(N100) / len(N100)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.5",
   "language": "python",
   "name": "3.8.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
