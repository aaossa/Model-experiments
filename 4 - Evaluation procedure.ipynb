{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import CuratorNet, CuratorNet2, VBPR\n",
    "from utils.data import extract_embedding\n",
    "from utils.metrics import (\n",
    "    auc_exact,\n",
    "    nDCG,\n",
    "    precision,\n",
    "    recall,\n",
    "    reciprocal_rank,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colaboratory setup\n",
    "\n",
    "Clone repository contents in VM and install dependencies using the script:\n",
    "\n",
    "```python\n",
    "# (1) Replace contents of VM\n",
    "!rm -rf sample_data\n",
    "# (Replace username and password/token)\n",
    "!git clone --single-branch --branch master https://username:password@github.com/aaossa/CuratorNet-experiments.git\n",
    "!cp -a CuratorNet-experiments/. .\n",
    "!rm -r CuratorNet-experiments/\n",
    "# Setup VM using script\n",
    "!chmod +x ./scripts/colaboratory.sh\n",
    "!./scripts/colaboratory.sh requirements/dev.txt\n",
    "```\n",
    "\n",
    "Mount Google Drive in case the data is available there:\n",
    "\n",
    "```python\n",
    "# (2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "\n",
    "Extract data in the right folder:\n",
    "\n",
    "```python\n",
    "# (3) Bring actual data to VM\n",
    "# Extract data from mounted drive to data folder\n",
    "!tar -xvzf \"/content/drive/My Drive/dataset/dataset.tar.gz\" -C data/dataset\n",
    "```\n",
    "\n",
    "**Important:** Restart the VM after following the steps to make sure you're using the right version of the declared requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# * UGallery\n",
    "# * Wikimedia\n",
    "# * Pinterest\n",
    "# * Tradesy\n",
    "DATASET = \"UGallery\"\n",
    "assert DATASET in [\"UGallery\", \"Wikimedia\", \"Pinterest\", \"Tradesy\"]\n",
    "\n",
    "# Model\n",
    "# * CuratorNet\n",
    "# * VBPR\n",
    "MODEL = \"CuratorNet\"\n",
    "assert MODEL in [\"CuratorNet\", \"CuratorNet2\", \"VBPR\"]\n",
    "\n",
    "# Feature extractor\n",
    "FEATURE_EXTRACTOR = \"resnet50\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode\n",
    "# Use 'MODE_PROFILE = True' for CuratorNet-like training \n",
    "# Use 'MODE_PROFILE = False' for VBPR-like training\n",
    "MODE_PROFILE = MODEL in [\"CuratorNet\", \"CuratorNet2\"]\n",
    "MODE_PROFILE = \"profile\" if MODE_PROFILE else \"user\"\n",
    "\n",
    "# Checkpoint (ex. 'CuratorNet_2020-08-07-23-59-50')\n",
    "CHECKPOINT = None\n",
    "if CHECKPOINT is not None:\n",
    "    assert CHECKPOINT.startswith(f\"{MODEL}_{DATASET}_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (general)\n",
    "CHECKPOINT_PATH = os.path.join(\"checkpoints\", MODEL, f\"{CHECKPOINT}.tar\")\n",
    "EMBEDDING_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}_embedding-{FEATURE_EXTRACTOR}.npy\")\n",
    "EVALUATION_PATH = os.path.join(\"data\", DATASET, f\"{MODE_PROFILE}-evaluation.csv\")\n",
    "\n",
    "# Paths (images)\n",
    "IMAGES_DIR = None\n",
    "if DATASET == \"UGallery\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"workspace\", \"Ugallery\", \"images\")\n",
    "elif DATASET == \"Wikimedia\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"wikimedia\", \"images\", \"img\")\n",
    "elif DATASET == \"Pinterest\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"pinterest_iccv\", \"images\")\n",
    "elif DATASET == \"Tradesy\":\n",
    "    print(\"Tradesy dataset not supported at the moment.\")\n",
    "\n",
    "# General constants\n",
    "RNG_SEED = 0\n",
    "USE_GPU = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed... ({RNG_SEED})\")\n",
    "    torch.manual_seed(RNG_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
    "embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "\n",
    "# Extract features and \"id2index\" mapping\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features, _, item_index2fn = extract_embedding(embedding, verbose=True)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "del embedding  # Release some memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataframe\n",
    "print(\"\\nLoad evaluation dataframe\")\n",
    "evaluation_df = pd.read_csv(EVALUATION_PATH)\n",
    "# Transform lists from str to int\n",
    "string_to_list = lambda s: list(map(int, s.split()))\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].apply(\n",
    "    lambda s: string_to_list(s) if isinstance(s, str) else s,\n",
    ")\n",
    "evaluation_df[\"predict\"] = evaluation_df[\"predict\"].apply(\n",
    "    lambda s: string_to_list(s) if isinstance(s, str) else s,\n",
    ")\n",
    "# Group evaluations by profile and user\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].map(tuple)\n",
    "evaluation_df = evaluation_df.groupby([\"profile\", \"user_id\"]).agg({\"predict\": sum}).reset_index()\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].map(list)\n",
    "print(f\">> Evaluation: {evaluation_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create device instance\n",
    "print(\"\\nDevice initialization\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "if torch.cuda.is_available() != USE_GPU:\n",
    "    print((f\"\\nNotice: Not using GPU - \"\n",
    "           f\"Cuda available ({torch.cuda.is_available()}) \"\n",
    "           f\"does not match USE_GPU ({USE_GPU})\"\n",
    "    ))\n",
    "\n",
    "# Loading checkpoint\n",
    "if CHECKPOINT is not None:\n",
    "    print(\"\\nLoading checkpoint\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(\"cpu\"))\n",
    "    print(f\">> Best epoch: {checkpoint['epoch']} | Best accuracy: {checkpoint['accuracy']}\")\n",
    "\n",
    "# Model initialization\n",
    "print(\"\\nModel initialization\")\n",
    "model = None\n",
    "if MODEL == \"CuratorNet\":\n",
    "    model = CuratorNet(\n",
    "        torch.Tensor(features),  # Pretrained visual features\n",
    "        input_size=features.shape[1],  # Network input size\n",
    "    ).to(device)\n",
    "elif MODEL == \"CuratorNet2\":\n",
    "    model = CuratorNet2(\n",
    "        torch.Tensor(features),  # Pretrained visual features\n",
    "        input_size=features.shape[1],  # Network input size\n",
    "    ).to(device)\n",
    "elif MODEL == \"VBPR\":\n",
    "    n_users = checkpoint[\"model\"][\"gamma_users.weight\"].size(0)\n",
    "    n_items = checkpoint[\"model\"][\"gamma_items.weight\"].size(0)\n",
    "    dim_gamma = checkpoint[\"model\"][\"gamma_users.weight\"].size(1)\n",
    "    dim_theta = checkpoint[\"model\"][\"theta_users.weight\"].size(1)\n",
    "    model = VBPR(\n",
    "        n_users, n_items,  # Number of users and items\n",
    "        torch.Tensor(features),  # Pretrained visual features\n",
    "        dim_gamma, dim_theta,  # Size of internal spaces\n",
    "    ).to(device)\n",
    "\n",
    "# Load state dict\n",
    "if CHECKPOINT is not None:\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# Change model mode to eval\n",
    "print(\"\\nChanging model mode to eval\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all\n",
    "# If True, ranks every item including already consumed items\n",
    "# If False, ranks ALL - PROFILE (consumed) + PREDICT (ground truth)\n",
    "PREDICT_ALL = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Metrics\n",
    "N_EVALS = len(evaluation_df.index)\n",
    "# Area Under the Curve (AUC)\n",
    "AUC = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "# Reciprocal Rank (RR)\n",
    "RR = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "# Recall\n",
    "R20 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "R100 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "R200 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "# Precision\n",
    "P20 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "P100 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "P200 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "# Normalized discounted cumulative gain (nDCG)\n",
    "N20 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "N100 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "N200 = torch.zeros([N_EVALS], dtype=torch.float64, device=device)\n",
    "PROFILE_SIZES = torch.zeros([N_EVALS], dtype=int, device=device)\n",
    "N_ITEMS = len(features)\n",
    "\n",
    "\n",
    "cache = model.generate_cache()\n",
    "\n",
    "\n",
    "evaluation_df[\"profile\"] = evaluation_df[\"profile\"].map(tuple)\n",
    "grouped_evals = evaluation_df.groupby([\"profile\", \"user_id\"]).agg({\"predict\": sum}).reset_index()\n",
    "for i, row in tqdm(enumerate(evaluation_df.itertuples()), total=len(evaluation_df.index)):\n",
    "    # Load data into tensors\n",
    "    profile = torch.tensor(row.profile).to(device, non_blocking=True).unsqueeze(0)\n",
    "    user_id = torch.tensor([int(row.user_id)]).to(device, non_blocking=True)\n",
    "    predict = torch.tensor(row.predict).to(device, non_blocking=True)\n",
    "    # Prediction\n",
    "    if MODE_PROFILE == \"profile\":\n",
    "        scores = model.recommend_all(profile, cache=cache)\n",
    "    elif MODE_PROFILE == \"user\":\n",
    "        scores = model.recommend_all(user_id, cache=cache).squeeze()\n",
    "    # Ranking\n",
    "    pos_of_evals = (torch.argsort(scores, descending=True)[..., None] == predict).any(-1).nonzero().flatten()\n",
    "    if not PREDICT_ALL:\n",
    "        pos_of_profi = (torch.argsort(scores, descending=True)[..., None] == profile).any(-1).nonzero().flatten()\n",
    "        # Relevant dimensions\n",
    "        _a, _b = pos_of_evals.size(0), pos_of_profi.size(0)\n",
    "        # Calculate shift for each eval item\n",
    "        shift = (pos_of_profi.expand(_a, _b) < pos_of_evals.reshape(_a, 1).expand(_a, _b)).sum(1)\n",
    "        # Apply shift\n",
    "        pos_of_evals -= shift.squeeze(0)\n",
    "    # Store metrics\n",
    "    AUC[i] = auc_exact(pos_of_evals, N_ITEMS)\n",
    "    RR[i] = reciprocal_rank(pos_of_evals)\n",
    "    R20[i] = recall(pos_of_evals, 20)\n",
    "    P20[i] = precision(pos_of_evals, 20)\n",
    "    N20[i] = nDCG(pos_of_evals, 20)\n",
    "    R100[i] = recall(pos_of_evals, 100)\n",
    "    P100[i] = precision(pos_of_evals, 100)\n",
    "    N100[i] = nDCG(pos_of_evals, 100)\n",
    "    R200[i] = recall(pos_of_evals, 200)\n",
    "    P200[i] = precision(pos_of_evals, 200)\n",
    "    N200[i] = nDCG(pos_of_evals, 200)\n",
    "    PROFILE_SIZES[i] = len(row.profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display stats\n",
    "print(f\"AVG AUC = {AUC.mean()}\")\n",
    "print(f\"AVG RR = {RR.mean()}\")\n",
    "print(f\"AVG R20 = {R20.mean()}\")\n",
    "print(f\"AVG P20 = {P20.mean()}\")\n",
    "print(f\"AVG NDCG20 = {N20.mean()}\")\n",
    "print(f\"AVG R100 = {R100.mean()}\")\n",
    "print(f\"AVG P100 = {P100.mean()}\")\n",
    "print(f\"AVG NDCG100 = {N100.mean()}\")\n",
    "print(f\"AVG R200 = {R200.mean()}\")\n",
    "print(f\"AVG P200 = {P200.mean()}\")\n",
    "print(f\"AVG NDCG200 = {N200.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def smart_group(value):\n",
    "    if value == 0:\n",
    "        return 0\n",
    "    digits = int(np.log10(value)) + 1\n",
    "    return (10**(digits - 1)) * (value // (10**(digits - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "metrics_data = [\n",
    "    [\n",
    "        PROFILE_SIZES[i].item(), AUC[i].item(), RR[i].item(),\n",
    "        R20[i].item(), P20[i].item(), N20[i].item(),\n",
    "        R100[i].item(), P100[i].item(), N100[i].item(),\n",
    "    ]\n",
    "    for i in range(N_EVALS)\n",
    "]\n",
    "metrics_df = pd.DataFrame(metrics_data, columns=[\n",
    "    \"PROFILE_SIZES\", \"AUC\", \"RR\",\n",
    "    \"R20\", \"P20\", \"N20\",\n",
    "    \"R100\", \"P100\", \"N100\",\n",
    "])\n",
    "metrics_df[\"PROFILE_SIZES_STEPS\"] = metrics_df[\"PROFILE_SIZES\"].map(smart_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Metric\n",
    "METRIC = \"AUC\"\n",
    "# Profile size range\n",
    "metrics_df_plot = metrics_df.copy()\n",
    "metrics_df_plot = metrics_df_plot[\n",
    "    (metrics_df_plot[\"PROFILE_SIZES_STEPS\"] >= 0) & (metrics_df_plot[\"PROFILE_SIZES_STEPS\"] < 100)\n",
    "]\n",
    "# Plot METRIC distribution across users grouped by profile size\n",
    "plt.figure(figsize=(24, 9))\n",
    "ax = sns.violinplot(x=\"PROFILE_SIZES_STEPS\", y=METRIC, data=metrics_df_plot, inner=None)\n",
    "if DATASET != \"Pinterest\":\n",
    "    ax = sns.swarmplot(x=\"PROFILE_SIZES_STEPS\", y=METRIC, data=metrics_df_plot, color=\"black\", edgecolor=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under the Curve distribution across users\n",
    "metrics_df[\"AUC\"].plot.box(sym=\"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First relevant item position (1 / reciprocal_rank) distribution across users\n",
    "# Line marks the 10% of the dataset\n",
    "graph = (1 / metrics_df[\"RR\"]).plot.box(sym=\"r+\")\n",
    "plt.ylim(0, features.shape[0])\n",
    "graph.axhline(features.shape[0] / 10, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First relevant item position (1 / reciprocal_rank) histogram\n",
    "graph = (1 / metrics_df[\"RR\"]).plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row in evaluation dataframe\n",
    "row = evaluation_df.iloc[ROW]\n",
    "\n",
    "# Load data into tensors\n",
    "profile = torch.tensor(row.profile).to(device, non_blocking=True).unsqueeze(0)\n",
    "user_id = torch.tensor([int(row.user_id)]).to(device, non_blocking=True)\n",
    "predict = torch.tensor(row.predict).to(device, non_blocking=True)\n",
    "# Prediction\n",
    "if MODE_PROFILE == \"profile\":\n",
    "    scores = model.recommend_all(profile)\n",
    "elif MODE_PROFILE == \"user\":\n",
    "    scores = model.recommend_all(user_id).squeeze()\n",
    "# Ranking\n",
    "pos_of_evals = (torch.argsort(scores, descending=True)[..., None] == predict).any(-1).nonzero().flatten()\n",
    "if not PREDICT_ALL:\n",
    "    pos_of_profi = (torch.argsort(scores, descending=True)[..., None] == profile).any(-1).nonzero().flatten()\n",
    "    pos_of_evals -= (pos_of_profi < pos_of_evals).sum()\n",
    "\n",
    "# Display metrics\n",
    "print(f\"| {'-' * 15} | {'-' * 7} |\")\n",
    "print(f\"| {'Metric':^15} | {'Score':^7} |\")\n",
    "print(f\"| {'-' * 15} | {'-' * 7} |\")\n",
    "print(f\"| {'AUC':^15} | {auc_exact(pos_of_evals, N_ITEMS):.5f} |\")\n",
    "print(f\"| {'RR':^15} | {reciprocal_rank(pos_of_evals):.5f} |\")\n",
    "for k in [20, 100, 500]:\n",
    "    print(f\"| {'-' * 15} | {'-' * 7} |\")\n",
    "    print(f\"| {f'Recall@{k}':^15} | {recall(pos_of_evals, k):.5f} |\")\n",
    "    print(f\"| {f'Precision@{k}':^15} | {precision(pos_of_evals, k):.5f} |\")\n",
    "    print(f\"| {f'nDCG@{k}':^15} | {nDCG(pos_of_evals, k):.5f} |\")\n",
    "print(f\"| {'-' * 15} | {'-' * 7} |\")\n",
    "\n",
    "# Profile and prediction\n",
    "profile = profile.cpu().numpy().flatten()\n",
    "predict = predict.cpu().numpy().flatten()\n",
    "# Ranking\n",
    "K = 20\n",
    "ranking = torch.argsort(scores, descending=True).cpu().numpy().flatten()\n",
    "if not PREDICT_ALL:\n",
    "    ranking = ranking[(~np.isin(ranking, profile)) | (np.isin(ranking, predict))]\n",
    "ranking = ranking[:K]\n",
    "print()\n",
    "print(f\"Size of profile: {profile.size}\")\n",
    "print(f\"Position of actual items: {pos_of_evals.cpu().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "COLUMNS = 10\n",
    "ELEMENTS = {\n",
    "    \"Consumed\": profile,\n",
    "    \"Recommendation\": ranking,\n",
    "    \"Ground truth\": predict,\n",
    "}\n",
    "SHOW_FILENAME = False\n",
    "\n",
    "for label, items in ELEMENTS.items():\n",
    "    n_rows = ((len(items) - 1) // COLUMNS + 1)\n",
    "    fig = plt.figure(figsize=(COLUMNS * 2, 4 * n_rows))\n",
    "    plt.title(f\"{label.title()} (n={len(items)})\")\n",
    "    plt.axis(\"off\")\n",
    "    for i, img_id in enumerate(items, start=1):\n",
    "        img_fn = item_index2fn[img_id]\n",
    "        image = mpimg.imread(os.path.join(IMAGES_DIR, img_fn))\n",
    "        ax = fig.add_subplot(n_rows, COLUMNS, i)\n",
    "        if SHOW_FILENAME:\n",
    "            ax.set_title(img_fn)\n",
    "        if label == \"Recommendation\":\n",
    "            if img_id in predict:\n",
    "                ax.patch.set_edgecolor(\"green\")\n",
    "                ax.patch.set_linewidth(\"5\")\n",
    "                if SHOW_FILENAME:\n",
    "                    ax.set_title(img_fn, color=\"green\")\n",
    "                else:\n",
    "                    ax.set_title(\"Ground truth\", color=\"green\")\n",
    "            elif img_id in profile:\n",
    "                ax.patch.set_edgecolor(\"red\")\n",
    "                ax.patch.set_linewidth(\"5\")\n",
    "                if SHOW_FILENAME:\n",
    "                    ax.set_title(img_fn, color=\"red\")\n",
    "                else:\n",
    "                    ax.set_title(\"Consumed\", color=\"red\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.5",
   "language": "python",
   "name": "3.8.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
