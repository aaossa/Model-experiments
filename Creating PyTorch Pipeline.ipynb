{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:53.542195Z",
     "start_time": "2020-04-06T15:20:53.367196Z"
    }
   },
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:54.037576Z",
     "start_time": "2020-04-06T15:20:53.545796Z"
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:55.936861Z",
     "start_time": "2020-04-06T15:20:54.053799Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install -r requirements/dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:56.169307Z",
     "start_time": "2020-04-06T15:20:55.944668Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls data/UGallery -sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:57.126593Z",
     "start_time": "2020-04-06T15:20:56.832845Z"
    }
   },
   "outputs": [],
   "source": [
    "# ugallery_data_utils.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_embeddings(embedding_path, embedding_shape=(13297, 2048)):\n",
    "    data = np.load(embedding_path, allow_pickle=True)\n",
    "    # Generate indexes and contiguous embedding\n",
    "    embedding = np.zeros(shape=embedding_shape)\n",
    "    artwork_id2index = dict()\n",
    "    artwork_index2id = dict()\n",
    "    for i, (artwork_id_hash, artwork_embedding) in enumerate(data):\n",
    "        assert artwork_id_hash not in artwork_id2index\n",
    "        artwork_id2index[artwork_id_hash] = i\n",
    "        assert i not in artwork_index2id \n",
    "        artwork_index2id[i] = artwork_id_hash\n",
    "        assert not np.any(embedding[i])\n",
    "        embedding[i] = artwork_embedding\n",
    "    assert not np.all(embedding == 0)\n",
    "    assert artwork_id2index\n",
    "    assert artwork_index2id\n",
    "    return embedding, artwork_id2index, artwork_index2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:58.366549Z",
     "start_time": "2020-04-06T15:20:57.128782Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler, SequentialSampler\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:58.400371Z",
     "start_time": "2020-04-06T15:20:58.369004Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset (1): Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from files\n",
    "embeddings_path = join(\"data\", \"UGallery\", \"ugallery_resnet50_embeddings.npy\")\n",
    "loaded_data = load_embeddings(embeddings_path)\n",
    "embeddings, artwork_id2index, artwork_index2id = loaded_data\n",
    "print(f\"embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(join(\"data\", \"UGallery\", \"artwork_id2index.json\"), \"r\") as file:\n",
    "    file_artwork_id2index = json.load(file)\n",
    "\n",
    "assert file_artwork_id2index == artwork_id2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset (1): Custom Dataset and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    \"\"\"Convert ndarrays in sample dict to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return {\n",
    "            k: torch.from_numpy(v).float()\n",
    "            for k, v in sample.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UGalleryDataset(Dataset):\n",
    "    # TODO(Antonio): Options for training, validation and testing. Training\n",
    "    # and validation are stored as csv files, but testing is a json file.\n",
    "    # Based on torchvision.Dataset maybe\n",
    "\n",
    "    def __init__(self, csv_file, embedding, transform=None):\n",
    "        # Dataframe\n",
    "        self.triples = pd.read_csv(csv_file)\n",
    "        profile_to_list = lambda p: p[1:-1].replace(\"'\", \"\").split(\", \")\n",
    "        self.triples[\"profile\"] = self.triples[\"profile\"].map(profile_to_list)\n",
    "        # Caching profile sizes\n",
    "        self.profile_sizes = tuple(self.triples[\"profile\"].map(len))\n",
    "        self.embedding = embedding\n",
    "        self.transform = transform\n",
    "        self.__ready = False\n",
    "        \n",
    "    def prepare(self, id2index=None):\n",
    "        if self.__ready:\n",
    "            raise Exception(\"Dataset was already prepared\")\n",
    "        if id2index:\n",
    "            self.__apply_mapping(id2index)\n",
    "        self.triples = self.triples.to_numpy()\n",
    "        self.__ready = True\n",
    "        print(\"Dataset is ready\")\n",
    "        \n",
    "    def __apply_mapping(self, id2index):\n",
    "        def map_id2index(element):\n",
    "            if type(element) is list:\n",
    "                return [id2index[e] for e in element]\n",
    "            else:\n",
    "                return id2index[element]\n",
    "        self.triples = self.triples.applymap(map_id2index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        profile = self.embedding[self.triples[idx, 0], :]\n",
    "        pi = self.embedding[self.triples[idx, 1]]\n",
    "        ni = self.embedding[self.triples[idx, 2]]\n",
    "\n",
    "        sample = {\n",
    "            \"profile\": profile,\n",
    "            \"pi\": pi,\n",
    "            \"ni\": ni,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset (3): Custom BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameProfileSizeBatchSampler(BatchSampler):\n",
    "\n",
    "    def __init__(self, sampler, batch_size, bump_rate=0.05, drop_last=False):\n",
    "        self.sampler = sampler\n",
    "        assert hasattr(self.sampler.data_source, \"profile_sizes\")\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.bump_rate = bump_rate\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_queue = defaultdict(list)\n",
    "        profile_sizes = self.sampler.data_source.profile_sizes\n",
    "        for idx in self.sampler:\n",
    "            p_size = profile_sizes[idx]\n",
    "            batch_queue[p_size].append(idx)\n",
    "            if len(batch_queue[p_size]) == self.batch_size:\n",
    "                batch, batch_queue[p_size] = batch_queue[p_size][:], []\n",
    "                yield batch\n",
    "                if random.random() < self.bump_rate and not self.drop_last:\n",
    "                    possible_keys = [k for k, v in batch_queue.items() if v]\n",
    "                    if possible_keys:\n",
    "                        bumped_key = random.choice(possible_keys)\n",
    "                        batch, batch_queue[bumped_key] = batch_queue[bumped_key][:], []\n",
    "                        yield batch\n",
    "        if not self.drop_last:\n",
    "            for k in random.sample(list(batch_queue.keys()), len(batch_queue)):\n",
    "                if batch_queue[k]:\n",
    "                    yield batch_queue[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            counter = Counter(self.sampler.data_source.profile_sizes)\n",
    "            n_samples = 0\n",
    "            for k, v in counter.items():\n",
    "                n_samples += (v + self.batch_size - 1) // self.batch_size\n",
    "            return n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset (4): collate_fn function (merge batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_samples(data):\n",
    "    elem = data[0]\n",
    "    batch = dict()\n",
    "    for key, value in elem.items():\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            numel = value.numel() * len(data)\n",
    "            storage = value.storage()._new_shared(numel)\n",
    "            out = value.new(storage)\n",
    "        else:\n",
    "            out = torch.zeros(len(data), *value.size())\n",
    "        batch[key] = torch.cat([b[key] for b in data], out=out).view(-1, *value.size())\n",
    "    target = torch.ones(len(data), 1, 1)\n",
    "    return batch, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (1): Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuratorNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Common section\n",
    "        self.selu_common1 = nn.Linear(2048, 200)\n",
    "        self.selu_common2 = nn.Linear(200, 200)\n",
    "        \n",
    "        # Profile section\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 200))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 200))\n",
    "        self.selu_pu1 = nn.Linear(200 + 200, 300)\n",
    "        self.selu_pu2 = nn.Linear(300, 300)\n",
    "        self.selu_pu3 = nn.Linear(300, 200)\n",
    "        \n",
    "        # Random weight initialization\n",
    "        self.reset_parameters()\n",
    "                \n",
    "    def forward(self, profile, pi, ni):\n",
    "        # Positive item\n",
    "        pi = F.selu(self.selu_common1(pi))\n",
    "        pi = F.selu(self.selu_common2(pi))\n",
    "        \n",
    "        # Negative item\n",
    "        ni = F.selu(self.selu_common1(ni))\n",
    "        ni = F.selu(self.selu_common2(ni))\n",
    "        \n",
    "        # User profile\n",
    "        profile = F.selu(self.selu_common1(profile))\n",
    "        profile = F.selu(self.selu_common2(profile))\n",
    "        profile = torch.cat((self.maxpool(profile), self.avgpool(profile)), dim=-1)\n",
    "        profile = F.selu(self.selu_pu1(profile))\n",
    "        profile = F.selu(self.selu_pu2(profile))\n",
    "        profile = F.selu(self.selu_pu3(profile))\n",
    "        \n",
    "        # x_ui > x_uj\n",
    "        x_ui = torch.bmm(profile, pi.unsqueeze(-1))  # .squeeze(2)\n",
    "        x_uj = torch.bmm(profile, ni.unsqueeze(-1))  # .squeeze(2)\n",
    "        \n",
    "        return x_ui - x_uj\n",
    "    \n",
    "    def recommend(self, profile, items, grad_enabled=False):\n",
    "        with torch.set_grad_enabled(grad_enabled):\n",
    "            # User profile\n",
    "            profile = F.selu(self.selu_common1(profile))\n",
    "            profile = F.selu(self.selu_common2(profile))\n",
    "            profile = torch.cat((self.maxpool(profile), self.avgpool(profile)), dim=-1)\n",
    "            profile = F.selu(self.selu_pu1(profile))\n",
    "            profile = F.selu(self.selu_pu2(profile))\n",
    "            profile = F.selu(self.selu_pu3(profile))\n",
    "\n",
    "            # Items\n",
    "            items = F.selu(self.selu_common1(items))\n",
    "            items = F.selu(self.selu_common2(items))\n",
    "\n",
    "            # x_ui\n",
    "            x_ui = torch.bmm(profile, items.transpose(-1, -2)).squeeze()\n",
    "\n",
    "            return x_ui\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Common section\n",
    "        nn.init.xavier_uniform_(self.selu_common1.weight)\n",
    "        nn.init.xavier_uniform_(self.selu_common2.weight)\n",
    "        # Profile section\n",
    "        nn.init.xavier_uniform_(self.selu_pu1.weight)\n",
    "        nn.init.xavier_uniform_(self.selu_pu2.weight)\n",
    "        nn.init.xavier_uniform_(self.selu_pu3.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (2): Include LR scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = {\n",
    "    \"dataloader:batch_size\": 4096 * 3, # * 2,\n",
    "    \"dataloader:num_workers\": 4, # 4,\n",
    "    \"training:num_epochs\": 150, # 300 is ideal\n",
    "    \"optimizer:lr\": 0.0001,  #  * 2, Had it like that the first time I think\n",
    "    \"optimizer:weight_decay\": 0.001,\n",
    "    \"scheduler:factor\": 0.6,\n",
    "}\n",
    "# double learning rate if you double batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer_name = \"CuratorNet_UGallery\"\n",
    "for k, v in SETTINGS.items():\n",
    "    print(k, v)\n",
    "    summary_writer_name = summary_writer_name + f\"_{k.split(':')[1]}={v}\"\n",
    "print(summary_writer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CuratorNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training criteria\n",
    "optimizer = optim.Adam(model.parameters(), lr=SETTINGS[\"optimizer:lr\"], weight_decay=SETTINGS[\"optimizer:weight_decay\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=SETTINGS[\"scheduler:factor\"], patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (training)\n",
    "training_dataset = UGalleryDataset(\n",
    "    csv_file=\"data/UGallery/train_public.csv\",\n",
    "    embedding=embeddings,\n",
    "    transform=transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ]))\n",
    "training_dataset.prepare(artwork_id2index)\n",
    "training_sampler = RandomSampler(training_dataset)\n",
    "training_batch_sampler = SameProfileSizeBatchSampler(sampler=training_sampler, batch_size=SETTINGS[\"dataloader:batch_size\"])\n",
    "training_dataloader = DataLoader(training_dataset, collate_fn=merge_samples, batch_sampler=training_batch_sampler, num_workers=SETTINGS[\"dataloader:num_workers\"], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (validation)\n",
    "validation_dataset = UGalleryDataset(\n",
    "    csv_file=\"data/UGallery/validation_public.csv\",\n",
    "    embedding=embeddings,\n",
    "    transform=transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ]))\n",
    "validation_dataset.prepare(artwork_id2index)\n",
    "validation_sampler = SequentialSampler(validation_dataset)\n",
    "validation_batch_sampler = SameProfileSizeBatchSampler(sampler=validation_sampler, bump_rate=0.0, batch_size=SETTINGS[\"dataloader:batch_size\"])\n",
    "validation_dataloader = DataLoader(validation_dataset, collate_fn=merge_samples, batch_sampler=validation_batch_sampler, num_workers=SETTINGS[\"dataloader:num_workers\"], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter(f\"runs/{summary_writer_name}\", flush_secs=20)\n",
    "\n",
    "\n",
    "def train_model(model, device, criterion, optimizer, scheduler, dataloaders, num_epochs=1, experiment_name=None):\n",
    "    model = model.to(device)\n",
    "    start = time.time()\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_validation_acc = 0.0\n",
    "    # Checkpoint\n",
    "    checkpoint_filename = f\"{model.__class__.__name__}_{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    checkpoint_filepath = join(\"checkpoints\", checkpoint_filename)\n",
    "    print(f\"Checkpoints stored at {checkpoint_filepath}\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"validation\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_acc = 0\n",
    "            running_x = 0\n",
    "                \n",
    "            # Iterate over data\n",
    "            for i_batch, (batch, target) in enumerate(tqdm(dataloaders[phase], desc=f\"Epoch {epoch} ({phase})\")):\n",
    "                batch = {\n",
    "                    k: v.to(device)\n",
    "                    for k, v in batch.items()\n",
    "                }\n",
    "                \n",
    "                target = target.to(device)\n",
    "                \n",
    "                # Restart params gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    output = model(**batch)\n",
    "                    loss = criterion(output, target)\n",
    "                    # Backward pass\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * output.size(0)\n",
    "                running_acc += (output.cpu().detach().numpy() > 0).sum()\n",
    "                running_x += output.size(0)\n",
    "                \n",
    "                if i_batch % 40 == 39:\n",
    "                    writer.add_scalar(\n",
    "                        f\"{phase} loss\",\n",
    "                        running_loss / running_x,\n",
    "                        (epoch - 1) * len(dataloaders[phase]) + i_batch,\n",
    "                    )\n",
    "                    writer.close()\n",
    "            \n",
    "            # Logging\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "            print(f\"{phase.title()} loss: {epoch_loss}\")\n",
    "            print(f\"{phase.title()} acc = {100 * epoch_acc}%\")\n",
    "            \n",
    "            # Deepcopy if model is good\n",
    "            if phase == \"validation\" and epoch_acc > best_validation_acc:\n",
    "                print(f\"New best model with ~{round(100 * epoch_acc, 4)}% acc ({epoch_acc})\")\n",
    "                best_validation_acc = epoch_acc\n",
    "                best_epoch = scheduler.last_epoch\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                torch.save({\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"validation_accuracy\": best_validation_acc,    \n",
    "                }, checkpoint_filepath)\n",
    "                print(f\"Saved model at {checkpoint_filepath}\")\n",
    "            \n",
    "            # Scheduler step if necessary\n",
    "            if phase == \"validation\":\n",
    "                print(f\"Scheduler: {scheduler.num_bad_epochs} bad epoch(s) (patience={scheduler.patience})\")\n",
    "                scheduler.step(epoch_acc)\n",
    "\n",
    "        print()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Training completed in {elapsed // 60:.0f}m {elapsed % 60:.0f}s\")\n",
    "    print(f\"Best validation accuracy: ~{round(100 * best_validation_acc, 4)}%\")\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_validation_acc, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (4): Cleverer... saving best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, validation_accuracy, best_epoch = train_model(\n",
    "    model, device,\n",
    "    criterion, optimizer, scheduler,\n",
    "    {\"train\": training_dataloader, \"validation\": validation_dataloader},\n",
    "    num_epochs=SETTINGS[\"training:num_epochs\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\n",
    "checkpoint_filename = f\"{model.__class__.__name__}_{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "torch.save({\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"epoch\": scheduler.last_epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    \"validation_accuracy\": validation_accuracy,    \n",
    "}, join(\"checkpoints\", checkpoint_filename))\n",
    "print(\"Saved model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
