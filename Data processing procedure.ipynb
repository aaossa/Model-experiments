{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install -r requirements/dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/UGallery -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "from os import cpu_count\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utils.ugallery.data import load_embedding, load_embedding_legacy, concatenate_embedding\n",
    "from utils.ugallery.entities import Inventory, User\n",
    "from utils.ugallery.hashing import HashesContainer\n",
    "from utils.ugallery.sampling import pre_hash, strategy_1, strategy_2, strategy_3, strategy_4, strategy_5, strategy_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating visual clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = {\n",
    "    \"clustering:n_times\": 20,\n",
    "    \"clustering:n_init\": 8,\n",
    "    \"clustering:n_jobs\": cpu_count(),\n",
    "    \"embeddings:public\": True,\n",
    "    \"inventory:public\": True,\n",
    "    \"output:index_mode\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_embedding_path = join(\"data\", \"UGallery\", \"ugallery_resnet50_embeddings.npy\")\n",
    "resnet50_legacy_embedding_path = join(\"data\", \"UGallery\", \"private\", \"flatten_1.npy\")\n",
    "resnet50_legacy_ids_path = join(\"data\", \"UGallery\", \"private\", \"ids\")\n",
    "resnext101_legacy_embedding_path = join(\"data\", \"UGallery\", \"private\", \"features.npy\")\n",
    "resnext101_legacy_ids_path = join(\"data\", \"UGallery\", \"private\", \"ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from the available files\n",
    "if SETTINGS[\"embeddings:public\"]:\n",
    "    EMBEDDINGS = {\n",
    "        \"ResNet50 (public)\": load_embedding(resnet50_embedding_path),\n",
    "    }\n",
    "else:\n",
    "    EMBEDDINGS = {\n",
    "        \"ResNet50 (private)\": load_embedding_legacy(resnet50_legacy_embedding_path, resnet50_legacy_ids_path),\n",
    "        \"ResNeXt-101 (private)\": load_embedding_legacy(resnext101_legacy_embedding_path, resnext101_legacy_ids_path),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_name, embedding_data in EMBEDDINGS.items():\n",
    "    print(f\"{embedding_name} embedding shape: {embedding_data.features.shape}\")\n",
    "\n",
    "print(f\"Merge {len(EMBEDDINGS)} embeddings into one...\")\n",
    "embedding = concatenate_embedding(EMBEDDINGS)\n",
    "print(f\"Merged embedding shape: {embedding.features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. z-score normalization of embedding\n",
    "embedding.features = StandardScaler().fit_transform(embedding.features)\n",
    "print(f\"z-score normalization shape: {embedding.features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Conduct PCA to reduce dimension\n",
    "embedding.features = PCA(n_components=200).fit_transform(embedding.features)\n",
    "print(f\"PCA reduction shape: {embedding.features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Perform k-means clustering with 100 clusters 20 times\n",
    "# and keep the clusterer with the highest Silhouette coefficient\n",
    "best_score = float(\"-inf\")\n",
    "best_clusterer = None\n",
    "\n",
    "for i in range(SETTINGS[\"clustering:n_times\"]):\n",
    "    clusterer = KMeans(\n",
    "        n_clusters=100,\n",
    "        max_iter=2000,\n",
    "        n_init=SETTINGS[\"clustering:n_init\"],\n",
    "        n_jobs=SETTINGS[\"clustering:n_jobs\"],\n",
    "    ).fit(embedding.features)\n",
    "    score = silhouette_score(embedding.features, clusterer.labels_)\n",
    "    if score > best_score:\n",
    "        best_clusterer = clusterer\n",
    "        best_score = score\n",
    "        print(f\"Silhouette score ({i + 1}): {score} - New highest!\")\n",
    "    else:\n",
    "        print(f\"Silhouette score ({i + 1}): {score}\")\n",
    "\n",
    "print(f\">> Best Silhouette score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Label each image with its respective visual cluster\n",
    "id2cluster = dict()\n",
    "cluster2id = defaultdict(list)\n",
    "for i, label in enumerate(best_clusterer.labels_):\n",
    "    artwork_id = embedding.index2id[i]\n",
    "    id2cluster[artwork_id] = label\n",
    "    cluster2id[label].append(artwork_id)\n",
    "\n",
    "n_clusters = len(set(id2cluster.values()))\n",
    "print(f\"There are n_clusters: {n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTINGS[\"inventory:public\"]:\n",
    "    inventory_path = join(\"data\", \"UGallery\", \"ugallery_inventory.csv\")\n",
    "    purchases_path = join(\"data\", \"UGallery\", \"ugallery_purchases.csv\")\n",
    "    inventory = Inventory(inventory_path, purchases_path)\n",
    "else:\n",
    "    inventory_path = join(\"data\", \"UGallery\", \"private\", \"valid_artworks.csv\")\n",
    "    purchases_path = join(\"data\", \"UGallery\", \"private\", \"valid_sales.csv\")\n",
    "    inventory = Inventory(inventory_path, purchases_path, legacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES_TRAIN = 10_000_000\n",
    "TOTAL_SAMPLES_VALID = TOTAL_SAMPLES_TRAIN * 0.05\n",
    "\n",
    "N_STRATEGIES = 6\n",
    "N_SAMPLES_PER_STRATEGY_TRAIN = ceil(TOTAL_SAMPLES_TRAIN / N_STRATEGIES)\n",
    "N_SAMPLES_PER_STRATEGY_VALID = ceil(TOTAL_SAMPLES_VALID / N_STRATEGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2artist = dict(\n",
    "    zip(\n",
    "        inventory.inventory[\"artwork_id\"],\n",
    "        inventory.inventory[\"artist_id\"],\n",
    "    ))\n",
    "artist2id = defaultdict(list)\n",
    "for artwork_id, artist_id in id2artist.items():\n",
    "    artist2id[artist_id].append(artwork_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.build_users(id2cluster, id2artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes_container = HashesContainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Predicting missing item in purchase basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_train = strategy_1(\n",
    "    N_SAMPLES_PER_STRATEGY_TRAIN,\n",
    "    inventory, hashes_container,\n",
    "    id2cluster, id2artist,\n",
    ")\n",
    "s1_validation = strategy_1(\n",
    "    N_SAMPLES_PER_STRATEGY_VALID,\n",
    "    inventory, hashes_container,\n",
    "    id2cluster, id2artist,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Predicting next purchase basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_train = strategy_2(\n",
    "    N_SAMPLES_PER_STRATEGY_TRAIN,\n",
    "    inventory, hashes_container,\n",
    "    id2cluster, id2artist,\n",
    ")\n",
    "s2_validation = strategy_2(\n",
    "    N_SAMPLES_PER_STRATEGY_VALID,\n",
    "    inventory, hashes_container,\n",
    "    id2cluster, id2artist,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Recommending visually similar artworks from favorite artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = strategy_3(\n",
    "    N_SAMPLES_PER_STRATEGY_TRAIN,\n",
    "    inventory, hashes_container,\n",
    "    id2cluster, id2artist, cluster2id, artist2id,\n",
    ")\n",
    "s3_validation = strategy_3(\n",
    "    N_SAMPLES_PER_STRATEGY_VALID,\n",
    "    inventory, hashes_container,\n",
    "    id2cluster, id2artist, cluster2id, artist2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Recommending profile items from the same user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4_train = strategy_4(\n",
    "    N_SAMPLES_PER_STRATEGY_TRAIN,\n",
    "    inventory, hashes_container,\n",
    ")\n",
    "s4_validation = strategy_4(\n",
    "    N_SAMPLES_PER_STRATEGY_VALID,\n",
    "    inventory, hashes_container,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Recommending profile items given an artificially created user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_train = strategy_5(\n",
    "    N_SAMPLES_PER_STRATEGY_TRAIN,\n",
    "    inventory, hashes_container,\n",
    ")\n",
    "s5_validation = strategy_5(\n",
    "    N_SAMPLES_PER_STRATEGY_VALID,\n",
    "    inventory, hashes_container,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Artificial profile with a single item: recommend visually similar items from the same artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6_train = strategy_6(\n",
    "    N_SAMPLES_PER_STRATEGY_TRAIN,\n",
    "    inventory, hashes_container,\n",
    "    id2artist, artist2id,\n",
    ")\n",
    "s6_validation = strategy_6(\n",
    "    N_SAMPLES_PER_STRATEGY_VALID,\n",
    "    inventory, hashes_container,\n",
    "    id2artist, artist2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total collisions: {hashes_container.collisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data\n",
    "\n",
    "Tuples will be scores as artwork indexes in the embedding instead of using the hashes, to improve performance and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge strategies samples\n",
    "TRAINING_DATA = [\n",
    "    s1_train,\n",
    "    s2_train,\n",
    "    s3_train,\n",
    "    s4_train,\n",
    "    s5_train,\n",
    "    s6_train,\n",
    "]\n",
    "# Transform samples from ids to indexes\n",
    "TRAINING_DATA = [\n",
    "    (triple[0], triple[1], triple[2])\n",
    "    for strategy_samples in TRAINING_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\"There are {len(TRAINING_DATA)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for duplicated hashes\n",
    "training_hash_check = HashesContainer()\n",
    "for triple in TRAINING_DATA:\n",
    "    assert training_hash_check.enroll(pre_hash(triple))\n",
    "print(\"No duplicated hashes found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ids to indexes\n",
    "if SETTINGS[\"output:index_mode\"]:\n",
    "    TRAINING_DATA = [\n",
    "        (\n",
    "            [embedding.id2index[i] for i in triple[0]],\n",
    "            embedding.id2index[triple[1]],\n",
    "            embedding.id2index[triple[2]],\n",
    "        )\n",
    "        for triple in TRAINING_DATA\n",
    "    ]\n",
    "print(\"Creating training output DataFrame\")\n",
    "df_train = pd.DataFrame(TRAINING_DATA, columns=[\"profile\", \"pi\", \"ni\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTINGS[\"embeddings:public\"]:\n",
    "    output_train = join(\"data\", \"UGallery\", \"train_public.csv\")\n",
    "else:\n",
    "    output_train = join(\"data\", \"UGallery\", \"train_private.csv\")\n",
    "df_train.to_csv(output_train, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge strategies samples\n",
    "VALIDATION_DATA = [\n",
    "    s1_validation,\n",
    "    s2_validation,\n",
    "    s3_validation,\n",
    "    s4_validation,\n",
    "    s5_validation,\n",
    "    s6_validation,\n",
    "]\n",
    "# Transform samples from ids to indexes\n",
    "VALIDATION_DATA = [\n",
    "    (triple[0], triple[1], triple[2])\n",
    "    for strategy_samples in VALIDATION_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\"There are {len(VALIDATION_DATA)} validation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for duplicated hashes\n",
    "validation_hash_check = HashesContainer()\n",
    "for triple in VALIDATION_DATA:\n",
    "    assert validation_hash_check.enroll(pre_hash(triple))\n",
    "\n",
    "print(\"No duplicated hashes found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ids to indexes\n",
    "if SETTINGS[\"output:index_mode\"]:\n",
    "    VALIDATION_DATA = [\n",
    "        (\n",
    "            [embedding.id2index[i] for i in triple[0]],\n",
    "            embedding.id2index[triple[1]],\n",
    "            embedding.id2index[triple[2]],\n",
    "        )\n",
    "        for triple in VALIDATION_DATA\n",
    "    ]\n",
    "print(\"Creating training output DataFrame\")\n",
    "df_validation = pd.DataFrame(VALIDATION_DATA, columns=[\"profile\", \"pi\", \"ni\"])\n",
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTINGS[\"embeddings:public\"]:\n",
    "    output_validation = join(\"data\", \"UGallery\", \"validation_public.csv\")\n",
    "else:\n",
    "    output_validation = join(\"data\", \"UGallery\", \"validation_private.csv\")\n",
    "df_validation.to_csv(output_validation, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data (evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTINGS[\"output:index_mode\"]:\n",
    "    evaluation_baskets = {\n",
    "        uid: {\n",
    "            \"profile\": [embedding.id2index[i] for i in user.profile],\n",
    "            \"evaluation_basket\": [embedding.id2index[i] for i in user.evaluation_basket],\n",
    "            \"evaluation_timestamp\": user.evaluation_timestamp,\n",
    "        }\n",
    "        for uid, user in inventory.users.items()\n",
    "        if user.evaluation_basket\n",
    "    }\n",
    "else:\n",
    "    evaluation_baskets = {\n",
    "        uid: {\n",
    "            \"profile\": [i for i in user.profile],\n",
    "            \"evaluation_basket\": [i for i in user.evaluation_basket],\n",
    "            \"evaluation_timestamp\": user.evaluation_timestamp,\n",
    "        }\n",
    "        for uid, user in inventory.users.items()\n",
    "        if user.evaluation_basket\n",
    "    }\n",
    "\n",
    "print(f\"There are {len(evaluation_baskets)} evaluation baskets/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTINGS[\"embeddings:public\"]:\n",
    "    output_evaluation = join(\"data\", \"UGallery\", \"evaluation_public.json\")\n",
    "else:\n",
    "    output_evaluation = join(\"data\", \"UGallery\", \"evaluation_private.json\")\n",
    "\n",
    "with open(output_evaluation, \"w\") as file:\n",
    "    json.dump(evaluation_baskets, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
