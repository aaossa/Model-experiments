{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:53.542195Z",
     "start_time": "2020-04-06T15:20:53.367196Z"
    }
   },
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:54.037576Z",
     "start_time": "2020-04-06T15:20:53.545796Z"
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:55.936861Z",
     "start_time": "2020-04-06T15:20:54.053799Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install -r requirements/dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:56.169307Z",
     "start_time": "2020-04-06T15:20:55.944668Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls data/UGallery -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T15:20:58.366549Z",
     "start_time": "2020-04-06T15:20:57.128782Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "import time\n",
    "# from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import UGalleryDataset\n",
    "from models import CuratorNet\n",
    "from samplers import SameProfileSizeBatchSampler\n",
    "from samplers.utils import merge_samples\n",
    "from transforms import DictToTensor\n",
    "from utils.ugallery.data import load_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = {\n",
    "    \"dataloader:batch_size\": 4096 * 3,\n",
    "    \"dataloader:num_workers\": 4,\n",
    "    \"training:num_epochs\": 150,\n",
    "    \"optimizer:lr\": 0.0001,\n",
    "    \"optimizer:weight_decay\": 0.001,\n",
    "    \"scheduler:factor\": 0.6,\n",
    "}\n",
    "# NOTE(Antonio): Double learning rate if you double batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from the available files\n",
    "resnet50_embedding_path = join(\"data\", \"UGallery\", \"ugallery_resnet50_embeddings.npy\")\n",
    "embedding = load_embedding(resnet50_embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CuratorNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training criteria\n",
    "optimizer = optim.Adam(model.parameters(), lr=SETTINGS[\"optimizer:lr\"], weight_decay=SETTINGS[\"optimizer:weight_decay\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=SETTINGS[\"scheduler:factor\"], patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (training)\n",
    "training_dataset = UGalleryDataset(\n",
    "    csv_file=\"data/UGallery/train_public.csv\",\n",
    "    embedding=embedding.features,\n",
    "    transform=transforms.Compose([\n",
    "        DictToTensor(),\n",
    "    ]))\n",
    "training_dataset.prepare(embedding.id2index)\n",
    "training_sampler = RandomSampler(training_dataset)\n",
    "training_batch_sampler = SameProfileSizeBatchSampler(sampler=training_sampler, batch_size=SETTINGS[\"dataloader:batch_size\"])\n",
    "training_dataloader = DataLoader(training_dataset, collate_fn=merge_samples, batch_sampler=training_batch_sampler, num_workers=SETTINGS[\"dataloader:num_workers\"], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (validation)\n",
    "validation_dataset = UGalleryDataset(\n",
    "    csv_file=\"data/UGallery/validation_public.csv\",\n",
    "    embedding=embedding.features,\n",
    "    transform=transforms.Compose([\n",
    "        DictToTensor(),\n",
    "    ]))\n",
    "validation_dataset.prepare(embedding.id2index)\n",
    "validation_sampler = SequentialSampler(validation_dataset)\n",
    "validation_batch_sampler = SameProfileSizeBatchSampler(sampler=validation_sampler, bump_rate=0.0, batch_size=SETTINGS[\"dataloader:batch_size\"])\n",
    "validation_dataloader = DataLoader(validation_dataset, collate_fn=merge_samples, batch_sampler=validation_batch_sampler, num_workers=SETTINGS[\"dataloader:num_workers\"], pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, criterion, optimizer, scheduler, dataloaders, num_epochs=1, summary_writer=None):\n",
    "    model = model.to(device)\n",
    "    start = time.time()\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_validation_acc = 0.0\n",
    "    # Checkpoint\n",
    "    checkpoint_filename = f\"{model.__class__.__name__}_{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    checkpoint_filepath = join(\"checkpoints\", checkpoint_filename)\n",
    "    print(f\"Checkpoints stored at {checkpoint_filepath}\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"validation\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_acc = 0\n",
    "            running_x = 0\n",
    "                \n",
    "            # Iterate over data\n",
    "            for i_batch, (batch, target) in enumerate(tqdm(dataloaders[phase], desc=f\"Epoch {epoch} ({phase})\")):\n",
    "                batch = {\n",
    "                    k: v.to(device)\n",
    "                    for k, v in batch.items()\n",
    "                }\n",
    "                \n",
    "                target = target.to(device)\n",
    "                \n",
    "                # Restart params gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    output = model(**batch)\n",
    "                    loss = criterion(output, target)\n",
    "                    # Backward pass\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * output.size(0)\n",
    "                running_acc += (output.cpu().detach().numpy() > 0).sum()\n",
    "                running_x += output.size(0)\n",
    "                \n",
    "                if i_batch % 40 == 39 and summary_writer:\n",
    "                    summary_writer.add_scalar(\n",
    "                        f\"{phase} loss\",\n",
    "                        running_loss / running_x,\n",
    "                        (epoch - 1) * len(dataloaders[phase]) + i_batch,\n",
    "                    )\n",
    "                    summary_writer.close()\n",
    "            \n",
    "            # Logging\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "            print(f\"{phase.title()} loss: {epoch_loss}\")\n",
    "            print(f\"{phase.title()} acc = {100 * epoch_acc}%\")\n",
    "            \n",
    "            # Deepcopy if model is good\n",
    "            if phase == \"validation\" and epoch_acc > best_validation_acc:\n",
    "                print(f\"New best model with ~{round(100 * epoch_acc, 4)}% acc ({epoch_acc})\")\n",
    "                best_validation_acc = epoch_acc\n",
    "                best_epoch = scheduler.last_epoch\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                torch.save({\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"validation_accuracy\": best_validation_acc,    \n",
    "                }, checkpoint_filepath)\n",
    "                print(f\"Saved model at {checkpoint_filepath}\")\n",
    "            \n",
    "            # Scheduler step if necessary\n",
    "            if phase == \"validation\":\n",
    "                print(f\"Scheduler: {scheduler.num_bad_epochs} bad epoch(s) (patience={scheduler.patience})\")\n",
    "                scheduler.step(epoch_acc)\n",
    "\n",
    "        print()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Training completed in {elapsed // 60:.0f}m {elapsed % 60:.0f}s\")\n",
    "    print(f\"Best validation accuracy: ~{round(100 * best_validation_acc, 4)}%\")\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_validation_acc, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SummaryWriter to track training progress\n",
    "summary_writer_name = \"CuratorNet_UGallery_\" + \"_\".join([f\"{k}={v}\" for k, v in SETTINGS.items()])\n",
    "summary_writer = SummaryWriter(f\"runs/{summary_writer_name}\", flush_secs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply training procedure\n",
    "validation_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "model, validation_accuracy, best_epoch = train_model(\n",
    "    model, device,\n",
    "    criterion, optimizer, scheduler,\n",
    "    {\"train\": training_dataloader, \"validation\": validation_dataloader},\n",
    "    # num_epochs=SETTINGS[\"training:num_epochs\"],\n",
    "    summary_writer=summary_writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save last model\n",
    "checkpoint_filename = f\"{model.__class__.__name__}_{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "torch.save({\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"epoch\": scheduler.last_epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    \"validation_accuracy\": validation_accuracy,    \n",
    "}, join(\"checkpoints\", checkpoint_filename))\n",
    "print(\"Saved model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
